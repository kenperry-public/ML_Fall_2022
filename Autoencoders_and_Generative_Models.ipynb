{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
    "\\def\\qr#1{\\mathcal{q}(#1)}\n",
    "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYek5uDM0gmo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AutoEncoder (AE): High Level\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The Deep Learning analog of Principal Components (PCA)</li>\n",
    "        <ul>\n",
    "            <li>Most of the lessons of AE apply equally to PCA</li>\n",
    "        </ul>\n",
    "        <li>Unsupervised: no labels (really semi-supervised)</li>\n",
    "        <li>Create \"synthetic features\" from the original set of features</li>\n",
    "        <li>May be able to use reduced set of synthetic features (dimensionality reduction)</li>\n",
    "        <li><b>Generative (vs Discriminative)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "biAjrkv4XJPi",
    "outputId": "89b91be6-062c-4145-de24-2cb79b81b94d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_vanilla.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An Autoencoder network has two parts\n",
    "- An Encoder, which takes input $\\x$ and \"encodes\" it into $\\z$\n",
    "- A Decoder, which takes the encoding $\\z$ and tries to reproduce $\\x$\n",
    "\n",
    "Each part has its own weights, which can be discovered through training, with examples\n",
    "- $\\langle \\X, \\y \\rangle = \\langle \\X, \\X \\rangle$\n",
    "\n",
    "That is: we are asking the output to be identical to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\z$ is an alternative latent representation of $\\x$.\n",
    "- Encoded by the Encoder\n",
    "- Inverted by the Decoder\n",
    "\n",
    "But\n",
    "when the dimension of $\\z$ is less than the dimension of $\\x$.\n",
    "- $\\z$ is a *bottle-neck*\n",
    "- the inversion by the Decoder will be imperfect\n",
    "\n",
    "$\\z$ becomes a *reduced-dimensionality* approximation of $\\x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is reminiscent of the dimensionality reduction of Principal Components Analysis (PCA).\n",
    "\n",
    "The *main difference* from PCA\n",
    "- PCA uses a *linear* transformation\n",
    "- NN can use *non-linear* transformations too\n",
    "    - PCA as a special case of AE\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYek5uDM0gmo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Autoencoders: Uses\n",
    "\n",
    "## Dimension reduction\n",
    "\n",
    "After training\n",
    "- we can discard the Decoder\n",
    "- use the Encoder output (synthetic features) as reduced dimension inputs to a *new* task\n",
    "    - Encoder weights are **frozen**: non-learnable when training new task\n",
    "   - It may be easier to solve the new task given $\\z$ rather than $\\x$\n",
    "       - have already discovered \"structure\" of $\\x$\n",
    "   - *Transfer Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder: Encoder + New head</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_encoder_new_head.jpg\" width=1200></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In PCA, we eliminated original features that were \"less important\"\n",
    "- i.e., explained variation among only a small fraction of the training set\n",
    "    - recall how we re-denominated explained variance in terms of \"number of features\"\n",
    "    \n",
    "There is no direct similar concept of feature importance in AE\n",
    "- other than minimizing a Loss function, which *may* wind up focusing on \"important\" features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer-wise pre-training with Autoencoders\n",
    "\n",
    "Autoencoders played a vital role in the development of Deep Learning:\n",
    "- They made it possible to train otherwise untrainable NN's.\n",
    "- Other innovations supplanted the need for AE's to assist training\n",
    "    - better initialization\n",
    "    - better activations functions\n",
    "    - normalization\n",
    " \n",
    "Although they are no longer needed for that purpose, it is interesting to see how (and why) they were used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a NN with $L$ layers that solves a Supervised Learning Problem\n",
    "- Training attempts to learn the weights of all layers simultaneously\n",
    "- *Layer wise pre-training* was an attempt\n",
    "    - to *initialize* the weights of each layer\n",
    "    - in succession\n",
    "    - so that the task of simultaneously solving for optimal weights had a better chance of succeeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea was to learn an initialization of $\\W_\\llp$, the weights of layer $l$.\n",
    "- After having learned the weights $\\W_{(l')}$ for all layers $l' \\lt l$.\n",
    "\n",
    "\n",
    "To initialize $\\W_\\llp$:\n",
    "- Train an AE that takes $\\x^\\ip$ as input\n",
    "- Using initialized weights $\\W_{(l')}$ for all layers $l' \\lt l$\n",
    "- Produces $\\tx^\\ip$ at layer $l$'s output $\\y_\\llp$\n",
    "\n",
    "So weight initializations were learned layer by layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the labels $\\y^\\ip$ *were not used* !\n",
    "- wouldn't be useful for the shallow NN\n",
    "\n",
    "It was thought\n",
    "- to be easier to learn the structure of the input $\\x$ independent of the labels\n",
    "- to be easier to learn $\\W_\\llp$ incrementally\n",
    "\n",
    "One the weights $\\W_\\llp$ were initialized via AE's\n",
    "- training of the Supervised Learning task had a better chance of succeeding\n",
    "- compared to any other initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders and Transfer Learning\n",
    "Today, autoencoders are useful for another purpose: Transfer Learning.\n",
    "\n",
    "If we can train an AE network to create features that are useful for reconstruction\n",
    "- it is possible\n",
    "that these features are useful for solving more complicated tasks.\n",
    "\n",
    "This was in essence what\n",
    "- Our dimension reduction example (replace the head) was doing\n",
    "- Layerwise Pre-training was attempting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So it is not uncommon to approach a complicated problem\n",
    "- by first constructing an autoencoder to\n",
    "come up with an alternate (and smaller) representation of the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that Autoencoders are *unsupervised*: they don't take labels.  \n",
    "\n",
    "So the encodings they produce\n",
    "stress syntactic similarity, rather than semantic similarity.\n",
    "\n",
    "Their use in Transfer Learning depends on the hope that inputs that are syntactically similar also\n",
    "have the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Denoising\n",
    "\n",
    "Very much like dimension reduction but with the assumption that\n",
    "- \"less important\" features are just random noise that is added to the true example\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder: Denoising</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_denoising.png\" width=1200></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Artificial Intelligence\n",
    "\n",
    "A less obvious use of AE (using the Decoder rather than the Encoder) is to *generate* examples.\n",
    "\n",
    "Most of the Machine Learning we have studied thus far is *discriminative*\n",
    "- $\\pr{\\hat{\\y}^\\ip | \\x^\\ip )}$\n",
    "    - e.g., classifier: discriminate among the possible classes $\\y^\\ip$, given example $\\x^\\ip$\n",
    "\n",
    "We can use the Decoder on *arbitrary* $\\z$ to *generate* a completely  new $\\x$:\n",
    "- $\\pr{ \\x^{(i')} | \\z^{(i')} }$ for some $i'$ not in training\n",
    "- *generate* a new example $i'$, in the domain of $\\x$, that was not encountered during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Generator</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_decoder.jpg\" width=800</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gHxF58P28q4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoder (AE): Details\n",
    "\n",
    "The *task* that trains an Autoencoder\n",
    "- Given input $\\x^\\ip$\n",
    "- Output of Encoder: $\\z^\\ip = E(\\x^\\ip)$\n",
    "- Output of Decoder: $\\tx^\\ip = D(\\z^\\ip)$\n",
    "- \"Target\": $\\x^\\ip$\n",
    "\n",
    "Both the Encoder and Decoder are parameterized (learnable parameters)\n",
    "- Goal: find the parameters such that \n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    " \\tx^\\ip = D(E(\\x))  & \\approx & \\x \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$\\z^\\ip = E(\\x^\\ip)$ is the latent representation of $\\x^\\ip$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jm35Tb5vkgI-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function\n",
    "\n",
    "The obvious loss functions compare the original $\\x^\\ip$ and reconstructed $\\tilde\\x^\\ip$ feature by feature:\n",
    "\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "$$\n",
    "\\loss^\\ip = \\sum_{j=1}^{|\\x|} { (\\x^\\ip_j - \\tx^\\ip_j)^2 }\n",
    "$$\n",
    "\n",
    "### Binary Cross Entropy\n",
    "\n",
    "For the special case where *each* original feature is in the range $[0,1]$ (e.g., an image)\n",
    "\n",
    "$$\n",
    "\\loss^\\ip = \\sum_{j=1}^{|\\x|} {  \\left( \\x^\\ip_j    \\log(\\tx^\\ip_j) + ( 1 - \\x^\\ip_j ) \\log(1 - \\tx^\\ip_j) \\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoder (VAE): Generative ML\n",
    "\n",
    "Observe that the Decoder part of the \"vanilla\" AE $D( \\z^\\ip )$\n",
    "- has been trained to produce \"realistic\" $\\tx^\\ip$ *only* for a $\\z^\\ip = E(\\x^\\ip)$\n",
    "    - i.e., \"realistic\": appears to come from the distribution of training $\\X$\n",
    "- there is no guarantee that $D( \\z^{(i')} )$ for some $i'$ not in training is realistic\n",
    "\n",
    "That is: the AE has not been trained to *extrapolate* beyond the training inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A VAE is able generate outputs \n",
    "- that *could have* come from the training distribution from a latent representation $\\z^{(i')}$ \n",
    "- but that *did not* come from $\\X$.\n",
    "\n",
    "Our goal is constructing a **Decoder** that can extrapolate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder will take a *latent vector* $\\z$ and produce $D(\\z)$, just as in a vanilla AE.\n",
    "\n",
    "\n",
    "The difference is that $\\z$ will be sampled from a *distribution* rather than being a \n",
    "unique mapping of a training example.\n",
    "\n",
    "This will be done by modifying the Encoder\n",
    "- It will *indirectly* create $\\z^\\ip$\n",
    "- It will compute *variables* $\\mu^\\ip$ and $\\sigma^\\ip$\n",
    "    - $\\z^\\ip$ will be *sampled* from a distribution with mean $\\mu^\\ip$ and standard deviations $\\sigma^\\ip$\n",
    "    \n",
    "As long as $\\z$ is sampled from this distribution, the decoder will produce a \"realistic\" output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "$\\mu$ and $\\sigma$ are \n",
    "- vectors\n",
    "- computed values (and hence, functions of $\\x$) and **not** parameters\n",
    "- so training learns a *function* from $\\x^\\ip$ to $\\mu^\\ip$ and $\\sigma^\\ip$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train a VAE:\n",
    "- pass input $\\x^\\ip$ through the Encoder, producing $\\mu^\\ip, \\sigma^\\ip$\n",
    "    - use $\\mu^\\ip, \\sigma^\\ip$ to sample a latent representation $\\z^\\ip$ from the distribution\n",
    "- pass the sampled $\\z^\\ip$ through the decoder, producing $D(\\z^\\ip)$\n",
    "- measure the reconstruction error $\\x^\\ip - D(\\z^\\ip)$, just as in a vanilla AE\n",
    "- back propagate the error, updating all weights and $\\mu, \\sigma$\n",
    "\n",
    "Essentially, each input $\\x^\\ip$ has *many* latent representations (with different probabilities):\n",
    "any sample from the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbWCKt7Or89C",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Training**\n",
    "\n",
    "Encoder produces\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "E(\\x) & = &  q_\\phi(\\z|\\x) & \\approx & p_\\theta(\\z|\\x) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We sample from\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\hat{\\z} & \\sim & q_\\phi(\\z|\\x) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Decoder produces\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "D(\\hat{\\z})  & = & p_\\theta (\\x|\\z)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Each time (epoch) that we encounter the same training example, we select another random element from the distribution.\n",
    "\n",
    "So the VAE learns to represent the same example from multiple latents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbWCKt7Or89C",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Generative**\n",
    "- sample $\\hat{\\z} \\sim \\hat{p}(\\z)$\n",
    "- use Decoder to produce output $p_\\theta (\\x|\\z)$\n",
    " \n",
    "This means we can feed in a $\\z$ \n",
    "- that doesn't correspond to any training example\n",
    "- and perhaps get an output that *resembles* something from the training set, rather than noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To give you an idea of the generative nature of the VAE, consider\n",
    "- Creating latent vectors $\\z$ from scratch\n",
    "    - **not** as the output of the Encoder\n",
    "- Varying these latent vectors systematically and examining the output created by the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "    <center>MNIST clustering produced by a VAE</center>\n",
    "    <br>\n",
    "<img src=images/VAE_examine_latent.png>\n",
    "    </div>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the outputs\n",
    "- are **not** instances of any examples\n",
    "- There was no guarantee that a random $\\z$ would produce something that looked like a digit !\n",
    "\n",
    "We may even be able to interpret the elements of $\\z$\n",
    "- $\\z_0$: control slant ?\n",
    "    - See the bottom row of $0$'s\n",
    "- $\\z_1$: control \"verticality\" ?\n",
    "    - See right-most column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFfhVRnM3C7q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional VAE\n",
    "\n",
    "Once a VAE is trained, $D(\\z)$ should produce a realistic output, for any $\\z$ from the distribution.\n",
    "\n",
    "However, if the distribution of $\\X$ includes examples from many classes \n",
    "- Assuming we have labels as auxilliary information (not used in training)\n",
    "    - e.g., the 10 digits\n",
    "- The VAE can't control *which class* the output will come from\n",
    "\n",
    "A *Conditional VAE* allow our generator (Decoder) to control the class $c$ of the output $\\tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Conditional VAE (CVAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_CVAE.jpg\"\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kaLKMwF5bOR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The class label $c$\n",
    "- is given as part of *training*\n",
    "    - So the Encoder produces a distribution that is conditioned on *both* $\\x$ and $c$.\n",
    "- is an *additional parameter* of the Decoder\n",
    "    - So the output class can be controlled\n",
    "$$\n",
    "\\tx^\\ip = D(\\z^\\ip, c)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kaLKMwF5bOR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So now we \n",
    "- create a latent $\\z$\n",
    "- append a class label $c$\n",
    "- and presumably have the decoder produce an output from the desired class.\n",
    "\n",
    "- The encoding distribution is now conditional on class label $c$: $q_\\phi(z|x,c)$ \n",
    "- So is the decoding distribution $p_\\theta(x|z,c)$ \n",
    "\n",
    "Again, by restricting the functional form of the prior distribution $\\hat{p}$ we can simplify the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detour: Autoencoder notebook on Colab\n",
    "\n",
    "Let's examine some Keras code that implements several types of Autoencoders\n",
    "- Vanilla\n",
    "- Denoising\n",
    "- VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will write our AE's using the Keras *Functional* API rather than the *Sequential* model\n",
    "- We *could* write the complete AE using the Sequential API\n",
    "- **But**\n",
    "    - we want to extract the Encoder and Decoder parts as *separate models*\n",
    "    - we can do this with the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will now switch to a notebook running on Google Colab\n",
    "<!--- #include (Autoencoder_example.ipynb)) --->\n",
    " [Autoencoder example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/Autoencoder_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VAE derivation: Probabilistic formulation\n",
    "\n",
    "**Note**: Advanced material\n",
    "\n",
    "Let's pretend: we don't already know that we will represent $\\mathbf{z}$ as a function of $\\mathbf{\\mu}_\\theta(\\x)$ and $\\mathbf{\\sigma}_\\theta(\\x)$\n",
    "- this derivation will show **why** we made that choice\n",
    "\n",
    "The mathematical derivation of a VAE is quite detailed\n",
    "- it is interesting but not absolutely necessary to understand\n",
    "- this is where we define the Loss function\n",
    "\n",
    "The interested reader is refered to a highly recommended [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf).\n",
    "\n",
    "We will try to give the essence in the following slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The VAE has a very interesting <b>two part</b> Loss Function</li>\n",
    "        <ul>\n",
    "            <li>Reconstruction Loss, as in the Vanilla AE</li>\n",
    "            <li>Divergence Loss\n",
    "        </ul>\n",
    "        <li>The Reconstruction Loss is not sufficient</li>\n",
    "        <ul>\n",
    "            <li>Issues of intractability arise</li>\n",
    "            <li>The Divergence Loss skirts intractability</li>\n",
    "            <ul>\n",
    "                <li>By constraining the Encoder to produce a tractable distribution</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can state our goal as\n",
    "- Producing $\\hat\\X^{(i')}$ that comes from the same distribution as training examples $\\X$\n",
    "- But are non-deterministic\n",
    "    - some $\\hat\\X^{(i')}$ are not exactly equal to $\\X^\\ip$ for any $1 \\le i \\le m$\n",
    "    \n",
    "That is: we want to generate new examples that are similar (but not identical) to the training examples $\\X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\pr{\\X}$ denote the probability distribution of training examples $\\X$.\n",
    "\n",
    "Note that $\\pr{\\X}$ is an *empirical* distribution define by the finite set $\\X$\n",
    "- We do not have a *closed form* expression for the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One way to conceive of achieving the goal is to condition $\\pr{\\X}$ on a random variable $\\z$\n",
    "and generate a synthetic example based on the random value\n",
    "$$\\pr{\\X | \\z}$$\n",
    "\n",
    "We will use a Neural Network with weights $\\theta$ to compute\n",
    "$$\\prs{\\X | \\z}{\\theta}$$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 1</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B_0.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Let's try to create an optimization objective function against which we choose our model weights.\n",
    "\n",
    "We will use Maximum Likelihood as the objective\n",
    "- Given the weights: how likely is the model to produce the training distribution $\\X$ ?\n",
    "- Recall that the likelihood of the set $\\X$ is the product of the probabilities that the model produces each $\\x \\in \\X$\n",
    "- So the log likehood is the sum of the log probabilities\n",
    "\n",
    "Since our practice is to minimize Loss (rather than maximize an objective function)\n",
    "we write our loss as (negative of log) likelihood\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss & = & - \\log( \\pr{\\X} )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Minimizing $\\loss$ is equivalent to maximizing likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because the generation of $\\hat{\\x}$ depends on a random variable $\\z$\n",
    "we marginalize $\\x$ over $\\z$\n",
    "\n",
    "$$\n",
    "\\pr{\\x} = \\int_{\\z \\in Q}{ \\pr{\\x | \\z} \\; \\pr{\\z} }\n",
    "$$\n",
    "\n",
    "where random $\\z$ comes from (as of yet unknown) distribution $Q$.\n",
    "\n",
    "That is: there are potentially many (and at least one) random choice that produces approximations of any training example.\n",
    "\n",
    "We want to train a Neural Network to produce randomized examples that are similar to examples in $\\X$.\n",
    "\n",
    "This means that, for each example in $\\X$, we have to sample from $\\pr{\\z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some obvious concerns\n",
    "- It may be very expensive to draw many samples from $Q$ for each training $\\x$\n",
    "- Moreover: It is likely that that there are many random choices from $Q$\n",
    "    - for which the generated $\\prs{\\x | \\z}{\\Theta}$\n",
    "    - is *unlike* any example in $\\X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's address these concerns by considering the joint distribution of $\\X$ and $\\z$\n",
    "$$\n",
    "\\pr{\\x,\\z}\n",
    "$$\n",
    "(from which we can compute $\\pr{\\z | \\x}$ via Bayes formula)\n",
    "\n",
    "We can improve our sampling by considering only those choices of $\\z$ that could generate $\\x$\n",
    "and re-write the objective as\n",
    "\n",
    "$$\n",
    "\\pr{\\x} = \\int_{ \\z \\in \\pr{\\z | \\x} } { \\pr{\\x | \\z} \\; \\pr{\\z} }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem is that $\\pr{\\z | \\x}$ is intractable !\n",
    "- The examples in $\\X$ are *deterministic*: they were not produced using a random $\\z$\n",
    "    - $\\X$ is defined as an empirical distribution, i.e., the training examples\n",
    "        - which weren't conditioned on $\\z$\n",
    "    - So we have *no data* on which to infer a joint distribution\n",
    "- So we don't know the joint distribution $\\pr{ \\x, \\z }$ or the conditional distribution $\\pr{ \\z | \\x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The solution is *approximate* the intractable $\\pr{\\z | \\x}$ with a tractable\n",
    "$\\qrs{\\z | \\x}{\\Phi}$\n",
    "- That is computed by a Neural Network (the \"Encoder\")\n",
    "- We will learn this distribution (parameterized by $\\Phi$) by training on $ \\X$.\n",
    "\n",
    "We need this approximate distribution $\\qrs{\\z | \\x}{\\Phi}$ to be as close to the true distribution $\\pr{\\z | \\x}$ as possible.\n",
    "\n",
    "That is: Using KL divergence as a measure of the difference between two distributions, we want to minimize\n",
    "\n",
    "$$\n",
    "\\KL( \\qrs{\\z | \\x}{\\Phi} \\; ||\\; \\pr{\\z | \\x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 2</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that prior to the need to approximate $\\pr{\\z | \\x }$, there was only a Decoder in our formulation of a goal of producing $\\hat{\\X}$ similar to true $\\X$!\n",
    "\n",
    "The Encoder (i.e., the Neural Network producing $\\qrs{\\z | \\x}{\\Phi}$) arose\n",
    "\n",
    "- Out of the need to compute $\\pr{\\z | \\x }$\n",
    "- Rather than a priori design considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We add the KL divergence to our loss function\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss  & = & - \\log(\\prs{\\x}{\\theta}) + \\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\pr{\\z | \\x}) \\\\\n",
    "& = & \\loss_R + \\loss_D\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "which now has two objectives\n",
    "- Reconstruction loss $\\loss_R$: maximize the likelihood (by minimizing the negative likelihood)\n",
    "- Divergence constraint $\\loss_D$: $q_\\phi(\\z|\\x)$ must be close to $p_\\theta(\\z | \\x))$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss_R & = & - \\log( p_\\theta(\\x ) ) \\\\\n",
    "\\loss_D & = & \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\pr{\\z | \\x} \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will show (in the next section: lots of algebra !) that the loss can be re-written as\n",
    "$$\n",
    "\\loss = - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi}}\\left( \\log( \\prs{\\x|\\z}{\\Theta} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi}  \\; ||\\;  \\pr{\\z} )\n",
    "$$\n",
    "\n",
    "This is *almost* identical to our original express for $\\loss$ except\n",
    "- Re-write \n",
    "$$\\log(p_\\theta(\\x)) = \n",
    "\\E_{z \\sim \\qrs{\\z|\\x}{\\Phi}}\\left( \\log( \\prs{\\x|\\z}{\\Theta} ) \\right)\n",
    "$$\n",
    "- the KL term becomes\n",
    "$$\n",
    " \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\pr{\\z} \\right)\n",
    "$$\n",
    "rather than the original\n",
    "$$\n",
    "\\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\pr{\\z | \\x} \\right)\n",
    "$$\n",
    "\n",
    "**The purpose of re-writing**: replace intractable $\\pr{\\z|\\x}$ with a tractable $\\pr{\\z}$ !\n",
    "- So we can have a Loss function with which we can train !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advanced: Obtain $\\loss$ by rewriting $\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\pr{\\z|\\x}$\n",
    "\n",
    "Let's derive a simpler expression for $\\loss$ by manipulating $\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\prs{\\z|\\x}{\\Theta})$:\n",
    "\n",
    "$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\pr{\\z | \\x}) &  = & \\sum_z{ \\qrs{\\z|\\x}{\\Phi}(\\log(\\qrs{\\z|\\x}{\\Phi} - \\log(\\pr{\\z | \\x}) } & \\text{def. of KL} \\\\\n",
    "&  = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log(\\qrs{\\z|\\x}{\\Phi} - \\log(\\pr{\\z | \\x}) \\right) & \\text{def. of }\\E \\\\\n",
    "&  = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } ( \\; \\log(\\qrs{\\z|\\x}{\\Phi}) \\\\ & & -\\left( \\; \\log( \\pr{\\x | \\z}) + \\log(\\pr{\\z}) - \\log(\\pr{\\x} \\right)    \\,   )  \\;\\;)&  \\text{Bayes theorem on } \\\\\n",
    " & & & \\log(\\prs{\\z|\\x}{\\Theta}) \\\\\n",
    "\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\pr{\\z | \\x}) \\\\ - \\log(\\pr{\\x}) & = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\; \\log(\\qrs{\\z|\\x}{\\Phi})  - \\left( \\log( \\prs{\\x | \\z}{\\Theta} ) + \\log( \\pr{\\z} ) \\right) \\;\\right) & \\text{ move } \\log(\\pr{\\x}) \\text{ to LHS} \\\\\n",
    " & = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\; - \\log( \\prs{\\x | \\z}{\\Theta} ) + ( \\; \\log(\\qrs{\\z|\\x}{\\Phi})  - \\log( \\pr{\\z} ) \\; )     \\; \\right) & \\text{re-arrange terms} \\\\\n",
    " & = & - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\prs{\\x | \\z}{\\Theta} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi} \\; ||\\;  \\pr{\\z} ) & \\text{def. of KL} \\\\\n",
    " \\loss & = & - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\prs{\\x | \\z}{\\Theta} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi} \\; ||\\;  \\pr{\\z} ) & \\text{since LHS} = \\loss \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The key step**:\n",
    "- Using Bayes Theorem to re-write\n",
    "$$\\log(\\qrs{\\z|\\x}{\\Phi} $$\n",
    "as\n",
    "$$\n",
    "\\log( \\pr{\\x | \\z}) + \\log(\\pr{\\z}) - \\log(\\pr{\\x} )\n",
    "$$\n",
    "- This allows us do away with intractable conditional probability $\\qrs{\\z|\\x}{\\Phi}$\n",
    "- In favor of unconditional probability $\\pr{\\z}$\n",
    "\n",
    "The LHS cannot be optimized via SGD (recall the tractability issue with  $\\pr{\\z|\\x}$).\n",
    "\n",
    "**But the RHS can be made tractable** giving a tractable choice of $\\pr{\\z}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choosing $\\pr{\\z}$\n",
    "\n",
    "So what distribution should we use for the prior $\\pr{\\z}$ ?\n",
    "- It should be differentiable, since we use Gradient Descent for optimization\n",
    "- It should be tractable with a closed form (such as a normal)\n",
    "- If we choose $\\pr{\\z}$ as normal, we can require $q_\\phi( \\z | \\x )$ to be normal too\n",
    "    - The KL divergence between two normals is an easy to compute function of their means and standard deviations.\n",
    "    - See [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf) Section 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-parameterization trick\n",
    "\n",
    "There is still one impediment to training.\n",
    "\n",
    "It involves the random choice of $\\z \\sim \\qrs{\\z|\\x}{\\Phi}$ in\n",
    "\n",
    "$$\n",
    "\\loss_R = \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\prs{\\x | \\z}{\\Theta} ) \\right)\n",
    "$$\n",
    "\n",
    "This is not a problem in the forward pass.\n",
    "\n",
    "But in the backward pass we need to compute\n",
    "$$\n",
    "\\frac{\\loss_R}{\\partial \\Theta}\n",
    "$$\n",
    "\n",
    "How do we back propagte through a random choice ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnFvZdJ1sV_e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"reparameterization trick\" redefines the random choice $\\z$ as\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\mathbf{z}  & = & \\mathbf{\\mu}_\\theta(\\x) + \\mathbf{\\sigma}_\\theta(\\x) * \\mathbf{\\epsilon} \\\\\n",
    "\\mathbf{\\epsilon} & \\sim & p(\\mathbf{z}) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In this formulation, the random variable $\\epsilon$ appears in a product term\n",
    "- We can differentiate the product with respect to $\\Theta$\n",
    "- $\\epsilon$ can be treated as a constant in $\\frac{\\partial \\epsilon}{\\partial \\Theta}$\n",
    "\n",
    "The Encoder design is now to produce\n",
    "(trainable parameters) $\\mu_\\Theta, \\sigma_\\Theta$\n",
    "- And $\\z$ indirectly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Reparameterization trick</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Reparameterization_trick.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This gets us to the  final picture of the VAE:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ELBo (Evidence-based Lower Bound)\n",
    "\n",
    "By re-writing the Loss, we removed the intractable term $\\pr{\\z|\\x}$\n",
    "\n",
    "It turns out that even this may not be necessary.\n",
    "\n",
    "For the truly interested reader:\n",
    "- The derivation uses a method known as *Variational Inference*.  See this \n",
    "[blog](https://mbernste.github.io/posts/variational_inference/) for a summary.\n",
    "- One can show that loss $\\loss$ is equal to $-1$ times the *ELBo* (Evidence Based Lower Bound)\n",
    "\n",
    "So if one knows how to maximize the [ELBo](https://mbernste.github.io/posts/elbo/), one can minimize the loss.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVIedxZelBZN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function: discussion\n",
    "\n",
    "Let's examine the role of $\\loss_R$ and $\\loss_D$ in the loss function $\\loss$.\n",
    "\n",
    "- What would happen if we dropped $\\loss_D$ ?\n",
    "    - We would wind up with a deterministic $\\z$ and collapse to a vanilla VAE\n",
    "    \n",
    "- What would happen if we dropped $\\loss_R$ ?\n",
    "    - the encoding approximation $\\qrs{\\z|\\x}{\\Phi}$ would be close to the empirical $\\pr{\\z | \\x}$ *in distribution*\n",
    "    - but two variables with the same distribution are not necessarily the same ?\n",
    "        - e.g., get a distribution $p$ by flipping a coin\n",
    "            - let distribution $q$ be a relabelling of $p$ by changing Heads to Tails and vice-versa\n",
    "            - $p$ and $q$ are equal in distribution but clearly different !\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Autoencoders_and_Generative_Models.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
