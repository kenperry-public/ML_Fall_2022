{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Keras: motivation\n",
    "\n",
    "We have been using the Keras Model API (mostly the `Sequential`) as a black box.\n",
    "\n",
    "But it is highly customizable\n",
    "- A `Model` is a class (as in Python object)\n",
    "- It implements methods such as\n",
    "    - `compile`\n",
    "    - `fit`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can change the behavior of a model in several ways\n",
    "- Arguments to some methods are objects; we can pass non-default functions/objects\n",
    "    - e.g., custom loss function\n",
    "- We can override these (and other) methods to make our models do new things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `Layer` is also an abstract class (Python) in Keras.\n",
    "\n",
    "Hence\n",
    "- We can create new layer types\n",
    "- We can override the methods of a given layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this module\n",
    "- we will\n",
    "illustrate techniques that you can use to customize your Layers/Models.\n",
    "- Illustrate the Functional model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functional model: the basics, illustrated by the Transformer\n",
    "\n",
    "The `Sequential` model\n",
    "- organizes layers as an ordered list\n",
    "- restricts the input to layer $(\\ll+1)$ to be the output of layer $\\ll$.\n",
    "\n",
    "The [`Functional` model](https://keras.io/guides/functional_api/)\n",
    "- imposes **no** ordering on layers\n",
    "- imposes **no** restriction on connect outputs of one layer to the input of another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the `Functional` model let's take a first look\n",
    "at model implementing a single `Transformer` block\n",
    "- we will revisit this code later to illustrate other concepts\n",
    "\n",
    "Here is the picture of a Transformer block\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=50%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are actually 3 models in this [cell](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=cadZM4xkYDon) we will visit !\n",
    "\n",
    "Let's examine each one and try to relate the actual code to our picture.\n",
    "\n",
    "**pay close attention to the difference between $\\bar\\h$ (Encoder states) and $\\y$ (Decoder outputs)**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_2.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the above picture of the Transformer:\n",
    "- The output sequence $\\bar\\h_{(1..\\bar{T})}$ (i.e., latent states) of the Encoder\n",
    "    - Used in Decoder-Encoder attention\n",
    "    - $|| \\bar\\h || = \\bar{T} = \\text{length of Transformer input}$\n",
    "- The prefix of the Decoder outputs generated up to time $\\tt$\n",
    "    - The Decoder output at time $(\\tt-1)$ is appended to the Decoder inputs available at time $\\tt$\n",
    "    - So the inputs are the Decoder outputs $\\y_{(1..T)}$\n",
    "        - $T$ is *full* length of Transformer output\n",
    "        - Causal (Masked) Attention is used to restrict the Decoder\n",
    "            - from attending at step $\\tt$ to any $\\y_\\tp$ where $\\tt > (\\tt-1)$\n",
    "            - Can't look at an output that hasn't been generated yet !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First model: the Encoder side of the Transformer\n",
    "\n",
    "The Encoder side of the transformer:\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This illustrates the pattern common to `Functional` models\n",
    "- The output of a layer is assigned to a variable (e.g., `encoder_inputs` has the value of the model's inputs)\n",
    "- The output of a layer is connected to the input of another layer via \"function call\" syntax\n",
    "    - e.g., `encoder_inputs` is applied as the input to the `PositionalEmbedding` layer\n",
    "    \n",
    "        x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The collection (not necessarily a sequence) of `Layer` calls defines a graph of function calls that maps\n",
    "`Model` inputs to outputs.\n",
    "\n",
    "To turn this collection into a `Model`\n",
    "- We define which function to feed Model inputs to\n",
    "- We define which function's outputs are the output of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we define the Encoder side (sub-model of the Transformer) of the Transformer via\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "    \n",
    "This defines `encoder` to be a `Model` with\n",
    "- input: `Layer` `encoder_inputs` (i.e., the `Input` layer)\n",
    "- output: `Layer` `encoder_outputs` (i.e., the `TransformerEncoder`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see the Encoder side in pictures via\n",
    "\n",
    "    tf.keras.utils.plot_model(\n",
    "        encoder,\n",
    "        show_shapes=True, \n",
    "    )\n",
    "<table>\n",
    "    <tr><th><strong>Encoder side of Transformer</strong></th></tr>\n",
    "    <tr><img src=\"images/nmt_encoder_plot_model.png\"></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**: the input and output of a `Model` *don't have to be* `Layer` types !\n",
    "\n",
    "There is also a model for the Decoder side of the Transformer in the cell we will visit:\n",
    "\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "- input: An **array**  of 2 `Layer` types -- `[ decoder_inputs, encoded_seq_inputs ]`\n",
    "- output: `Layer`: `decoder_outputs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the Decoder side takes a **pair** of inputs, as per the diagram.\n",
    "\n",
    "    \n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "Let's see if we can trace which role each element of the pair serves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is  picture of the Decoder side of the Transformer\n",
    "\n",
    "<table>\n",
    "    <tr><th><strong>Decoder side of Transformer</strong></th></tr>\n",
    "    <tr><img src=\"images/nmt_decoder_plot_model.png\"></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note in the Decoder picture**\n",
    "\n",
    "`decoder_state_inputs` (on the right hand side) is assigned to variable `encoded_seq_inputs`:\n",
    "\n",
    "    encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    \n",
    "and `encoded_seq_inputs` is the second argument to `TransformerDecoder`\n",
    "\n",
    "Hence, there should be an arrow from `decoder_state_inputs` to the `transformer_decoder` box.\n",
    "\n",
    "This confirms that\n",
    "- The Decoder sides takes a pair of inputs\n",
    "- The second input is directed to the second argument of `TransformerDecoder`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's examine the Encoder code more closely\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, observe that the *Encoder* outputs ($\\bar\\y_{(1..\\bar{T})}$) `encoder_outputs`\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "We see that these Encoder outputs become the second part of the pair of the actual arguments that are the inputs to the *Decoder*\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    \n",
    "And the formal argument of `decoder` definition is `encoded_seq_inputs`\n",
    "\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the `encoder_outputs` ($\\bar\\y_{(1..\\bar{T})}$) become bound to `encoded_seq_inputs` within the Decoder.\n",
    "\n",
    "Hence, the second part of the input pair of `decoder` serves the role of $\\bar\\y_{(1..\\bar{T})}$, the sequence of Encoder latent states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Second model: The Decoder side of the Transformer\n",
    "\n",
    "Now, let's look at the Decoder.\n",
    "\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By tracing variable `x` backwards from the Decoder output ($\\y_{(1..T)}$) `decoder_outputs`\n",
    "\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "we can see the outputs derive from `Layer` sub-type `TransformerDecoder` \n",
    "\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "\n",
    "We have already shown that `encoded_seq_inputs` corresponds to $\\bar\\y_{(1..\\bar{T})}$\n",
    "\n",
    "So we can guess that variable `x` in the call to `TransformerDecoder` corresponds to $\\y_{(1..T)}$\n",
    "\n",
    "Let's confirm that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tracing variable `x` backwards from its use a the first argument in the `TransformerDecoder`\n",
    "\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "    \n",
    "we see that it is the positionally-encoded (to enable Causal masking) `decoder_inputs` \n",
    "- The `PositionalEmbedding` is added to enforce Masking (causal ordering)\n",
    "\n",
    "Hopefully: `decoder_inputs` are the `decoder_outputs` shifted by one time step\n",
    "- That is: the training set enforces \"Teacher Forcing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Third model: the full Transformer -- Encoder + Decoder\n",
    "\n",
    "Finally, there is the Transformer Model, combining an Encoder and Decoder:\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = keras.Model(\n",
    "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `transformer` input is a pair\n",
    "\n",
    "    [encoder_inputs, decoder_inputs]\n",
    "\n",
    "And it's output is\n",
    "\n",
    "     decoder_outputs\n",
    "\n",
    "We identify the first part of the input pair (`encoder_inputs`) as the input sequence $\\x_{(1\\dots \\bar{T})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the picture of the full Transformer:\n",
    "<table>\n",
    "    <tr><th><strong>Full Transformer</strong></th></tr>\n",
    "    <tr><img src=\"images/nmt_transformer_plot_model.png\"></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Note: Where does \"Teacher Forcing\" happen ?\n",
    "\n",
    "We had \"hoped\" that \n",
    ">`decoder_inputs` are the `decoder_outputs` shifted by one time step\n",
    "\n",
    "- which needs to be true at training time to implement \"teacher forcing\"\n",
    "- (but which shouldn't happen at inference time)\n",
    "\n",
    "Where in the code does this happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the data preparation,\n",
    "we can see that each row of the training data is mapped by the function\n",
    "\n",
    "    def format_dataset(eng, spa):\n",
    "        eng = eng_vectorization(eng)\n",
    "        spa = spa_vectorization(spa)\n",
    "        return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
    "\n",
    "where `eng` is $\\x_{1\\ldots\\bar{T}}$ and `spa` is $\\y_{1\\ldots T}$\n",
    "\n",
    "The function replaces the (\"feature\", \"target\") example pairs\n",
    "- denoted as (`eng`, `spa`) in the code (n.b., the \"feature\" is a sentence in English; the \"target\" a sentence in Spanish)\n",
    "\n",
    "by\n",
    "- example features are the hash `({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],},`\n",
    "- example target is now `spa[:, 1:]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So `decoder_inputs` would seem to be the un-shifted sequence of words in the Spanish sentence.\n",
    "\n",
    "But wait !\n",
    "\n",
    "Careful examination of the Spanish sentences show that they begin with a `[start]` token\n",
    "- This, in essence, has shifted the `decoder_input` by one word\n",
    "- So the first output of the Decoder will be the first (non-`[start]`) word of Spanish\n",
    "- Which will be compared to the new target `spa[:, 1:]`, which is the first (non-`[start]`) word of Spanish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At inference time, we do not see Teacher Forcing:\n",
    "\n",
    "We can see the generated output  `decoded_sentence` built up one word at a time\n",
    "in the code snippet for `decode_sequence`\n",
    "\n",
    "    def decode_sequence(input_sentence):\n",
    "        tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "        decoded_sentence = \"[start]\"\n",
    "        for i in range(max_decoded_sentence_length):\n",
    "            tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
    "            predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "              ....\n",
    "              \n",
    "            decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "            if sampled_token == \"[end]\":\n",
    "                break\n",
    "        return decoded_sentence\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "A lot going on here !\n",
    "- A complex connection of `Layer` outputs to inputs\n",
    "- Custom `Layer` sub-types\n",
    "    - `PositionalEmbedding`, `TransformerEncoder`, `TransformerDecoder`\n",
    "    - We will soon see how to define our own `Layer` sub-classes\n",
    "- Teacher Forcing\n",
    "\n",
    "That concludes our first look at the Functional model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model specialization\n",
    "\n",
    "## Custom loss (passing in a loss function)\n",
    "\n",
    "In introducing Deep Learning, we have asserted that\n",
    ">It's all about the Loss function\n",
    "\n",
    "That is: the key to solving many Deep Learning problems\n",
    "- Is not in devising a complex network architecture\n",
    "- But in writing a Loss function that captures the semantics of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Up until now\n",
    "- We have been  using pre-defined Loss functions (e.g., `binary_crossentropy`)\n",
    "- Specifying the Loss function in the compile statement\n",
    ">`model.compile(loss='binary_crossentropy')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can [write your own loss functions](https://keras.io/api/losses/)\n",
    "\n",
    "In Keras, a Loss function has the signature\n",
    ">`loss_fn(y_true, y_pred, sample_weight=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom train step (override `train_step`)\n",
    "\n",
    "But what if your Loss function needs access to values that are not part of the signature ?\n",
    "\n",
    "Or what if you want to change the training loop ?\n",
    "\n",
    "You could write your own training loop by overriding the `fit` method\n",
    "- Cycle through epochs\n",
    "- Within each epoch, cycle through mini-batches of examples\n",
    "- For each mini-batch of examples: execute the *train step*\n",
    "    - forward pass: feed input examples to Input layer, obtain output\n",
    "    - compute the loss\n",
    "    - Compute the gradient of the loss with respect to the weights\n",
    "    - Update the weights\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "initialize(W)\n",
    "\n",
    "# Training loop to implement mini-batch SGD\n",
    "for epoch in range(n_epochs):`\n",
    "    for X_batch, y_batch in next_batch(X_train, y_train, batch_size, shuffle=True):\n",
    "        # Forward pass\n",
    "        y = NN(X_batch)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = loss_fn(y, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = gradient(loss, W)\n",
    "        \n",
    "        # Update \n",
    "        W = W - grads * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than overriding `fit`, it sometimes suffices to override the train step: `train_step`\n",
    "\n",
    "Let's start by looking at the \"standard\" implementation of a basic train step.\n",
    "\n",
    "We will see\n",
    "- How losses are computed\n",
    "- Gradients are obtained\n",
    "- Weights are updated\n",
    "\n",
    "[Basic `train_step`](https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/customizing_what_happens_in_fit.ipynb#scrollTo=9022333acaa7&line=1&uniqifier=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can modify the basic training step too.\n",
    "\n",
    "For example: suppose we want to make some training examples \"more important\" than others\n",
    "- Rather than Total Loss as equally-weighted average over all examples\n",
    "- Pass in per-example weights\n",
    "\n",
    "This might be useful, for example, when dealing with Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Layer` specialization\n",
    "\n",
    "A `Layer` in Keras is an abstract (Python) object\n",
    "- instantiating the object returns a function\n",
    "    - That maps input to the layer to the output\n",
    "\n",
    "We have used specific instances of `Layer` objects (e.g., `Dense`) as arguments in the list passed to the `Sequential` model type.\n",
    "\n",
    "We can also use instances in the Functional Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- `Dense(10)`\n",
    "    - Is the constructor for a fully connected layer instance with 10 units\n",
    "    - The constructor returns a function\n",
    "    - The the function maps the layer inputs to the outputs of the computation defined by the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So you will see code fragments like\n",
    ">\n",
    "    x = Input(shape=(784))\n",
    "    x = Dense(10, activation=softmax)(x)\n",
    "    \n",
    "- Re-using the variable `x` as the output of the current layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the function is invoked, the Layer's `call` method is used\n",
    "- `call` gets invoked implicitly by \"parenthesized argument\" juxtaposition\n",
    "    - e.g., `Dense(10) ( x )`\n",
    "    - is similar to `obj= `Dense(10); result = obj.call(x)`\n",
    "- The function maps the inputs to the layer to the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Overriding `call` allows us to defined a new `Layer` sub-class.\n",
    "\n",
    "For example, [here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=AupqfNAYaCHn&line=2&uniqifier=1) is the code defining some new `Layer` types that will be used to create a `Transformer` layer type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of `Dense(10)` is a Tensor with final dimension equal to the number of units (e.g., 10)\n",
    "- The Tensor has leading dimensions too\n",
    "    - e.g., the implicit \"batch index\" dimension\n",
    "    - since the layer takes a mini-batch of examples (rather than a single example) as input\n",
    "- It may have *additional* dimensions too !\n",
    "    - Just like `numpy`: threading over additional dimensions\n",
    "    - e.g., if input is shape $(\\text{minibatch_size} \\times n_1 \\times n_2)$\n",
    "        - output is shape $(\\text{minibatch_size} \\times n_1 \\times 10)$\n",
    "        - `Dense` operates over the final dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Studying advanced models\n",
    "\n",
    "The best way to learn is to study the code of some non-trivial models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer: Custom layers, Skip connections, Layer Norm\n",
    "\n",
    "We have already seen part of the Transformer in introducing the basics of the Functional model.\n",
    "\n",
    "We use the rest of this example to discover other advanced Keras techniques:\n",
    "\n",
    "[Transformer layer](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=4DaEQr-lMkSs)\n",
    "- [Custom layers](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=Ywxggf9Anabk&line=8&uniqifier=1)\n",
    "- Layer Norm\n",
    "- Skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Custom layers: subtle point\n",
    "\n",
    "Let's look at the constructor for the `TransformerEncoder` custom layer, as an example\n",
    "\n",
    "    class TransformerEncoder(layers.Layer):\n",
    "        def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "            super(TransformerEncoder, self).__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.dense_dim = dense_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.attention = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim\n",
    "            )\n",
    "            self.dense_proj = keras.Sequential(\n",
    "                [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "            )\n",
    "            self.layernorm_1 = layers.LayerNormalization()\n",
    "            self.layernorm_2 = layers.LayerNormalization()\n",
    "            self.supports_masking = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The custom layer consists of a collection of component layers\n",
    "\n",
    "Why are the components layers (e.g., Dense, MultiHeadAttention, LayerNormalization) instantiated in the class constructor\n",
    "- As opposed to being defined in the `call` method \n",
    "\n",
    "Had we instantiated each component within the `call` method\n",
    "- There would be a *new instance* of each component *each time the layer was called on an example* in training !\n",
    "- Each instance would have *it's own weights*\n",
    "- So training would not \"learn\" between examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other custom layers of interest\n",
    "\n",
    "We can dig deeper to examine how the Attention layers are implemented in code:\n",
    "- [Scaled dot-product attention](https://www.tensorflow.org/text/tutorials/transformer#scaled_dot_product_attention)\n",
    "- [Multi-head attention](https://www.tensorflow.org/text/tutorials/transformer#multi-head_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VAE: Custom Model -- Custom Layer, Training Loop, the Gradient Tape\n",
    "\n",
    "We use this example to show\n",
    "- The Functional model\n",
    "- A custom `Layer`: `Sampling`\n",
    "- A custom training step\n",
    "- Inverting a Convolution: `Conv2DTranspose`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the architecture of a Variational Autoencoder (VAE)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A key step is drawing a random latent vector $\\z^\\ip$\n",
    "from a distribution with mean $\\mu$ and standard deviation $\\sigma$.\n",
    "\n",
    "This [cell](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=Wx_jzzcPtcfz)\n",
    "- Creates a custom `Layer` type called `Sampling` to perform the random sampling\n",
    "- By overriding the base `Layer` `call` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In [this cell](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=4rFiHtbCtcf0) we can see that the Encoder and Decoder are both implemented as Functional models\n",
    "- The Encoder produces a *pair of outputs*: $(\\mu, \\sigma)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Issues**\n",
    "- Custom **model** (not layer) class VAE\n",
    "- The *reconstruction loss* depends on the output of the Decoder part of the VAE\n",
    "    - No other obvious way to define this loss aside from a **custom training** step\n",
    "- Because we are computing the Loss in the training step\n",
    "    - we must compute the gradient of the Loss w.r.t weights\n",
    "    - Update the weights (**gradient tape**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Variational Autoencoder (VAE) from github](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=DEU05Oe0vJrY)\n",
    "- [VAE: Custom train step](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=0EHkZ1WCHw9E)\n",
    "    - Complex loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing what CNN's learn: Gradient Ascent and the Gradient Tape\n",
    "\n",
    "[Visualizing what Convnets learn](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb#scrollTo=K8ITQAj7FTZd)\n",
    "- The Gradient Tape\n",
    "- Maximize utility (negative loss)\n",
    "    - mean (across the spatial dimensions) of one feature map in a multi-layer CNN\n",
    "    - the \"weights\" being solved for are the pixels of the input image !\n",
    "\n",
    "We use this example to show how powerful the Gradient Tape is\n",
    "\n",
    "[Gradient ascent](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb#scrollTo=a9hZnslRFTZZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Factor Models and Autoencoders: Threading\n",
    "\n",
    "We use this example to show\n",
    "- A Functional model applied to a common problem in Finance.\n",
    "- Threading\n",
    "\n",
    "We will cover the Finance aspects of this in a [separate module](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "\n",
    "For now, I want to focus on the idea and the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the code, excerpted from the [notebook](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "\n",
    "    def make_model(hidden_units=8, n_factors=3):\n",
    "        input_beta = Input((n_tickers, n_characteristics), name='input_beta')\n",
    "        input_factor = Input((n_tickers,), name='input_factor')\n",
    "\n",
    "        hidden_layer = Dense(units=hidden_units, activation='relu', name='hidden_layer')(input_beta)\n",
    "        batch_norm = BatchNormalization(name='batch_norm')(hidden_layer)\n",
    "\n",
    "        output_beta = Dense(units=n_factors, name='output_beta')(batch_norm)\n",
    "\n",
    "        output_factor = Dense(units=n_factors, name='output_factor')(input_factor)\n",
    "\n",
    "        output = Dot(axes=(2,1), name='output_layer')([output_beta, output_factor])\n",
    "\n",
    "        model = Model(inputs=[input_beta, input_factor], outputs=output)\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not obvious what is going on here.\n",
    "\n",
    "A picture will help:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder for Conditional Risk Factors</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_for_conditional_risk_factors.png\" width=\"90%\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\R}{\\mathbf{R}}\n",
    "\\newcommand{\\r}{\\mathbf{r}}\n",
    "\\newcommand{\\F}{\\mathbf{F}}\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\ntickers}{{n_\\text{tickers}}}\n",
    "\\newcommand{\\ndates}{{n_\\text{dates}}}\n",
    "\\newcommand{\\nfactors}{{n_\\text{factors}}}\n",
    "\\newcommand{\\nchars}{{n_\\text{chars}}}\n",
    "\\newcommand{\\dp}{{(d)}}\n",
    "\\newcommand{\\sp}{{(s)}}\n",
    "\\newcommand{\\Bbeta}{\\mathbf\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Threading \n",
    "\n",
    "Let's focus on the `Dense` layer corresponding to the box labelled \"Beta\" in the picture\n",
    "\n",
    "- `Dense` $( \\nfactors )$\n",
    "\n",
    "From the diagram you will notice that \n",
    "- the input to this  layer is *two dimensional*: $(\\ntickers \\times \\nchars)$\n",
    "- the output to this is *two dimensional*: $(\\ntickers \\times \\nfactors)$\n",
    "\n",
    "We have not yet seen multi-dimensional input/output in regard to a `Dense` layer\n",
    "\n",
    "What is going on here ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The layer is implementing a function with signature\n",
    "- `Dense`( $\\nfactors ) :  (\\ntickers \\times \\nchars) \\mapsto (\\ntickers \\times \\nfactors) $\n",
    "\n",
    "Tensorflow/Keras works on higher dimensional objects just like NumPy: \n",
    "- [threading](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) over \"extra\" dimensions\n",
    "\n",
    "If the input to layer $\\ll$ is shape $(\\dim_{\\llp,1} \\times \\dim_{\\llp,2} \\times \\ldots \\dim_{\\llp,N} \\,\\, \\times n_\\llp )$\n",
    "- And the layer type operates over a *single* dimension (usually the last dimension)\n",
    "    - producing output shape $n_{(\\ll+1)}$    \n",
    "\n",
    "Then threading treats the inputs\n",
    "- as a tensor of shape $(\\dim_{\\llp,1} \\times \\dim_{\\llp,2} \\times \\ldots \\dim_{\\llp,N} )$ instances, each of shape $n_\\llp $\n",
    "\n",
    "Producing an output of shape \n",
    "- If the input to layer $\\ll$ is shape $(\\dim_{\\llp,1} \\times \\dim_{\\llp,2} \\times \\ldots \\dim_{\\llp,N} \\,\\, \\times n_{(\\ll+1)} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our case\n",
    "- Input shape is $(\\ntickers \\times \\nchars)$\n",
    "- The `Dense` layer is defined with $\\nfactors$ units ($n_{(\\ll+1)} = \\nfactors$)\n",
    "- Hence, the output shape is $(\\ntickers \\times \\nfactors)$\n",
    "\n",
    "The weight matrix for this layer\n",
    "- $\\W_\\beta$ with shape $( \\nfactors \\times \\nchars )$\n",
    "    - just like any `Dense` layer; number of weights is *independent* of threading\n",
    "- applies the *same weights* to each of the $\\ntickers$ (the rows) of the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Style Transfer: Feature extractor, Training Loop\n",
    "\n",
    "We use this example to illustrate\n",
    "- [Complex Loss and Training Loop](https://keras.io/examples/generative/neural_style_transfer/#compute-the-style-transfer-loss)\n",
    "- [Feature extractor](https://keras.io/examples/generative/neural_style_transfer/#compute-the-style-transfer-loss)\n",
    "\n",
    "[Here](https://www.tensorflow.org/tutorials/generative/style_transfer) is a tutorial view of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoder: Functional model\n",
    "\n",
    "\n",
    "<!--- #include (Autoencoder_example.ipynb)) --->\n",
    " [Autoencoder example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2022/blob/master/Autoencoder_example.ipynb)\n",
    "- Functional model\n",
    "\n",
    "**Issues**\n",
    "- We could use a Sequential model with initial Encoder layers and final Decoder layers\n",
    "    - But we would not be able to independently access the Encoder nor the Decoder as isolated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GAN\n",
    "\n",
    "We use this example to show\n",
    "- A custom training step\n",
    "- Inverting a Convolution: `Conv2DTranspose`\n",
    "\n",
    "Recall: the training of a GAN is an iterative process among two \"players\"\n",
    "- the Discriminator\n",
    "- the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Custom `train_step`\n",
    "\n",
    "Here is a summary from our introductory module on [GANs](GAN_Overview.ipynb)\n",
    "\n",
    "**Competitive training**\n",
    "\n",
    "Iteration $\\tt$\n",
    "\n",
    "- Train $D_{\\Theta_{D, (\\tt-1)}}$ on samples\n",
    "    - $\\tilde{\\x} \\in p_\\text{data} \\cup p_{\\text{model}, (\\tt-1)}$\n",
    "        - where $G_{\\Theta_{G, (\\tt-1)}} ( \\z) \\in p_{\\text{model}, (\\tt-1)}$\n",
    "    - Update $\\Theta_{D, (\\tt-1)}$ to $\\Theta_{D, \\tp}$ via gradient $\\frac{\\partial \\loss_D}{\\partial \\Theta_{D,(\\tt-1)}}$\n",
    "        - $D$ is a maximizer of $\\int_{\\x \\in p_\\text{data}} \\log D(\\x) + \\int_{\\z \\in p_\\z} \\log ( \\, 1 - D(G(\\z)) \\, )$\n",
    "- Train $G_{\\Theta_{G, (\\tt-1)}}$ on random samples $\\z$\n",
    "    - Create samples $\\hat{\\x}_\\tp \\in G_{\\Theta_{G, (\\tt-1)}}(\\z)  \\in p_\\text{model}$\n",
    "    - Have Discriminator $D_{\\Theta_{D, \\tp}}$ evaluate $D_{\\Theta_{D,\\tp}} ( \\hat{\\x}_\\tp )$\n",
    "    - Update $\\Theta_{G, (\\tt-1)}$ to $\\Theta_{G, \\tp}$ via gradient $\\frac{\\partial \\loss_G}{\\partial \\Theta_{G,(\\tt-1)}}$\n",
    "        - $G$ is a minimizer of $\\int_{\\z \\in p_\\z} \\log ( \\, 1 - D(G(\\z)) \\, )$\n",
    "            - i.e., want $D(G(\\z))$ to be high\n",
    "    - May update $G$ multiple times per update of $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Keras, one can override a `Model`'s `train_step` method in order to replace the\n",
    "treatment of a single mini-batch of examples.\n",
    "\n",
    "Let's see how this is applied to a [simple GAN](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb#scrollTo=OzzFLmKIpv0j)\n",
    "\n",
    "Key points to observe;\n",
    "- Discriminator trained first\n",
    "    - Create examples from real and fake images, to be fed to Discriminator\n",
    "    \n",
    "          # Decode them to fake images\n",
    "          generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "          # Combine them with real images\n",
    "          combined_images = tf.concat([generated_images, real_images], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Train the Discriminator on the combined real/fake images and update it's weights\n",
    "\n",
    "            # Train the discriminator\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.discriminator(combined_images)\n",
    "                d_loss = self.loss_fn(labels, predictions)\n",
    "            grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(grads, self.discriminator.trainable_weights)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Train the Generator\n",
    "    - Have it create fake images from random, latent vectors\n",
    "    - Let the Discriminator evaluate these fakes\n",
    "    - Update Generator weights to better be able to fool Discriminator\n",
    "    \n",
    "    \n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `Conv2DTranspose`: Inverting a Convolution\n",
    "\n",
    "A CNN layer\n",
    "- creates new features, for each element of the spatial dimension of the layer input\n",
    "- May \"down-sample\" the input (reduce spatial dimension)\n",
    "    - Using a stride greater than 1\n",
    "    \n",
    "We can invert the Convolution, and \"up-sample\" (increase the spatial dimension)\n",
    "- with the `Conv2DTranspose` layer type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can see the Discriminator using Convolutional layers with down-sampling\n",
    "- And the Generator using transposed Convolutional layers with up-sampling\n",
    "\n",
    "in this [cell](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb#scrollTo=OrfSmOevpv0h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Simple GAN](https://keras.io/examples/generative/dcgan_overriding_train_step)\n",
    "- [Custom train step: GAN training](https://keras.io/examples/generative/dcgan_overriding_train_step/#override-trainstep)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wasserstein GAN with Gradient Penalty\n",
    "[Wasserstein GAN with Gradient Penalty](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)\n",
    "- [Gradient Tape: used for loss term, rather than weight update](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)\n",
    "- [Overide `compile`](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)\n",
    "- [Custom train step: GAN training](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
