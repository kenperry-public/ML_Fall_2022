{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN in action: Understanding sequences\n",
    "\n",
    "We will study a toy example that is typical of many tasks involving sequences\n",
    "- Given a prefix of a sequence\n",
    "- Predict the next element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- Predict the next word in a sentence\n",
    "- Predict the next price in a timeseries of prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Being able to predict the next element may be key to understanding the \"logic\" underlying a sequence\n",
    "- You have to understand context and domain\n",
    "- You have to understand how earlier elements influence latter elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predict the next:  Data preparation\n",
    "\n",
    "It is our belief that Machine Learning is a *process* and not just a collection of models.\n",
    "\n",
    "We have recently been emphasizing the models but let's review the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/EdX/W10_L2_ML_process.png\"</td> <!Derived by EdX from images/ML_process.jpg>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is usually the case that Sequence data involves substantial Data Preparation.\n",
    "\n",
    "Suppose our task is to predict the next word in a sentence.\n",
    "\n",
    "We are given (or must obtain) a collection of sentences (e.g., one or more documents) as our raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But a sentence is not the format required for the training set of the \"Predict the next word\" task.\n",
    "\n",
    "Data preparation is usually a substantial prerequisite for solving tasks involving sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be precises, the \"Predict the next word\" task involves\n",
    "- Training a many to one RNN with examples created from a sequence.\n",
    "- The elements of a single example are the prefix of a sentence\n",
    "- The target of the example is the next word in the sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let \n",
    "$$[ \\;\\mathbf{s}_\\tp | 1 \\le \\tt \\le T \\; ]$$\n",
    "be the sequence of words in sentence $\\mathbf{s}$.\n",
    "\n",
    "We will prepare $(T-1)$ examples from this single sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\langle \\X, \\y \\rangle = $\n",
    "\n",
    "$\n",
    "\\begin{array} \\\\\n",
    "  i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "  \\hline \\\\\n",
    "  1 & \\mathbf{s}_{(1) }  & \\mathbf{s}_{(2)} \\\\\n",
    "  2 & \\mathbf{s}_{(1), (2) }  & \\mathbf{s}_{(3)} \\\\\n",
    "  \\vdots \\\\\n",
    "  i & \\mathbf{s}_{(1), \\ldots, (i) }  & \\mathbf{s}_{(i+1)} \\\\\n",
    "  \\vdots \\\\\n",
    "  (T-1) & \\mathbf{s}_{(1), \\ldots, (T-1) }  & \\mathbf{s}_{(T)} \\\\\n",
    "  \\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "\n",
    "$\\mathbf{s} = $\n",
    "\"I am taking a class in Machine Learning\"\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{array}\\\\\n",
    "i & \\x^\\ip  & \\y^\\ip \\\\\n",
    "1 & [\\;  \\text{I} \\; ] & \\text{am} \\\\\n",
    "2 & [\\; \\text{I, am} \\; ] & \\text{taking} \\\\\n",
    "3 & [\\; \\text{I, am, taking} \\; ] & \\text{a} \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of examples are the basis of **Semi-Supervised** Learning\n",
    "- *Supervised*: labeled examples\n",
    "- but labels are derived from the raw input\n",
    "    - which become features in the subsequent example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict the next: data shape\n",
    "\n",
    "We had warned earlier about the explosion of the number of dimensions of our data.\n",
    "Now is a good time to take stock\n",
    "- $\\X$, the training set, is a matrix with $m$ rows\n",
    "- Each row is an example $\\x^\\ip$\n",
    "- Each example is a sequence $[ \\; \\x^\\ip_\\tp \\, | \\, 1 \\le \\tt \\le || \\x^\\ip || \\; ]$\n",
    "- Each element $\\x^\\ip_\\tp$ of the sequence encodes a word\n",
    "- A word is encoded as a One Hot Encoded binary vector of length $|| V||$ where $V$ is the set of words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Target $\\y^\\ip$ is also a word (so is  vector of length $|| V||$).\n",
    "- Many to one: target is *not* a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predict the next: training\n",
    "\n",
    "Just like training any other type of layer, but more expensive\n",
    "- Each example involves multiple time steps: forward pass is time consuming\n",
    "- The derivatives (needed for Gradient Descent) are more complex; backward pass complex and time consuming\n",
    "\n",
    "Remember:\n",
    "- the target $\\y_\\tp$ for step $t$ should be $\\x_{(\\tt+1)}$ the next input\n",
    "$$\n",
    "\\y_\\tp = \\x_{(\\tt+1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Predict the next: Training</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a generative model (fun with RNN's)\n",
    "\n",
    "The \"Predict the next\" word task is interesting on its own\n",
    "- But a slight twist will make it extremely interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we have trained our model on a large collection of sentences of the same type (e.g., same author).\n",
    "\n",
    "At test time, we feed a short \"seed\" sentence\n",
    "$$\n",
    "\\x_{(0)}, \\ldots, \\x_\\tp\n",
    "$$\n",
    "into the model and have it generate output.\n",
    "\n",
    "**But** \n",
    "we then feed the output back into the model as input !\n",
    "$$\n",
    "\\x_{(\\tt'+1)} = \\y_{(\\tt')} \\; \\text{for } \\tt' \\ge \\tt\n",
    "$$\n",
    "\n",
    "- as in the Decoder in our Language Translation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_one_to_many.png\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model would generate new text ad infinitum\n",
    "- The next word generated would be based on what the model has learned from training\n",
    "- To be the most probable word to follow the prefix\n",
    "\n",
    "Voila: the RNN can *generate* text in the same style as the training sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using Machine Learning to *create* data is called *generative*.\n",
    "\n",
    "Using Machine Learning to classify/predict (as we've been doing thus far) is called *discriminative*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating strange things\n",
    "\n",
    "Generating stories from seeds was very popular a few years back.\n",
    "\n",
    "Let's look at some examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But first, a surprise:\n",
    "- Rather than solving a \"predict the next word\" task\n",
    "- All of the following examples were generated by a \"predict the next **character**\" task !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is somewhat amazing that what is generated\n",
    "- Has correctly spelled words/keywords\n",
    "- Is Syntactically correct (sentences end with a \".\", parentheses/brackets are balanced)\n",
    "- Is meaningful: the elements/words are arranged in a logical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even though\n",
    "- We have not explicilty identified any of these concepts\n",
    "- Nor forced training to respect them (via a loss function)\n",
    "\n",
    "Remember\n",
    "- All of this behavior was \"learned\" by identifying the correct next **character**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fake [Shakespeare](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#shakespeare), or fake politician-speak\n",
    "- Fake code \n",
    "- Fake [math textbooks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#algebraic-geometry-latex)\n",
    "- [Click bait headline generator](http://clickotron.com/about)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the generative model\n",
    "\n",
    "Let's describe this generative process in more detail.\n",
    "\n",
    "First: training a model.\n",
    "\n",
    "At test-time (when we are generating new text) the outputs are fed back as next-step inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_one_to_many.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But that's **not** how the model is trained\n",
    "- the \"next step\" input is the **true** next element in the sequence\n",
    "- this is how we construct the training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training, with Teacher Forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_many_to_many_teacher_forcing.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called *teacher forcing*\n",
    "\n",
    "$$\n",
    "\\x^\\ip_\\tp = \\y_{(t-1)}\n",
    "$$\n",
    "rather than\n",
    "$$\n",
    "\\x^\\ip_\\tp = \\hat\\y_{(t-1)}\n",
    "$$\n",
    "for $t > t'$.\n",
    "\n",
    "- When extending the sequence\n",
    "- A teacher forces the student (model) to continue with the *correct* answer\n",
    "- Rather than the student's answer\n",
    "- If it didn't do so, once the student (model) predicted incorrectly, it's errors would compound\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling from the generative model\n",
    "\n",
    "Remember that a Classifier (the output stage of our model)\n",
    "- generates a *probability distribution* (over the elements of the vocabulary $V$)\n",
    "\n",
    "For the prediction, we usually *deterministically* choose the element of $V$ with highest probability\n",
    "$$\n",
    "\\hat{\\y} = \\argmax{v \\in V} { \\pr{v} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Deterministic choice might not be best for the generative process\n",
    "- One wrong choice propagates to all successive elements of the sequence\n",
    "- The output is always the same ! Boring !\n",
    "\n",
    "So what is usually done is that our prediction is a *sample* from the probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Here is the process in pictures\n",
    "- The training inputs are given in red\n",
    "- The test (inference) time inputs are given in black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Teacher forcing is indicated in red\n",
    "- Predictions $[ \\; \\hat{\\y}_\\tp \\, | \\, 1 \\le \\tt \\le T \\; ]$ **are not** used as input (lower right)\n",
    "- Only correct targets $[ \\; \\y_\\tp \\, | \\, 1 \\le \\tt \\le T \\; ]$ are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The input sequence to the Decoder is modified by\n",
    "- prepending a special \"start of output\" symbol\n",
    "$$\\x_{(-1)} = \\langle \\text{START} \\rangle$$\n",
    "- appending a special \"end of output\" symbol $\\langle \\text{END} \\rangle$ to training examples\n",
    "    - The Decoder stops when it generates the end of output symbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Encoder* is a many to one RNN\n",
    "- Takes the variable length \"seed\" sequence\n",
    "- Outputs a fixed length representation of the seed\n",
    "    - This is one of the strengths of an RNN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Decoder* is a one to many RNN\n",
    "- Takes the fixed length representation of the seed produced by the Encoder\n",
    "    - Used to initialized the Decoder's latent state $\\h_{(0)}$\n",
    "- Outputs a variable length sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative text: state of the art\n",
    "\n",
    "The model we described (and will explore in a code example) is absolutely primitive\n",
    "- Predict next *character* rather than next *word*\n",
    "- Simple: one RNN layer\n",
    "- Trained on very small number of examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Here](https://app.inferkit.com/demo) is a link to a state of the art model.\n",
    "\n",
    "We will learn about its architecture in a later module.\n",
    "- 3 billion weights !\n",
    "- Trained on 500 billion tokens\n",
    "    - Used \\\\$ 42K of electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
