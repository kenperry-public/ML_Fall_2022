{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) layer\n",
    "\n",
    "**Review**\n",
    "\n",
    "With a sequence $\\x^\\ip$ as input, and a sequence $\\y$ as a potential output,  the questions arises:\n",
    "- How does an RNN produce $\\y_\\tp$, the $t^{th}$ output ?\n",
    "\n",
    "Some choices\n",
    "- Predict $\\y_\\tp$ as a direct function of the prefix of $\\x$ of length $\\tt$: \n",
    "$$\\pr{\\y_\\tp | \\x_{(1)} \\dots \\x_\\tp} $$\n",
    "\n",
    "- Uses a \"latent state\" that is updated with each element of the sequence, then predict the output\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\h_\\tp | \\x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } [ \\x_{(1)} \\dots \\x_\\tp ]\\\\\n",
    "\\pr{\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our first encounter with the RNN, we made the choice to use the \"latent state\" approach.\n",
    "\n",
    "Doing so enabled us to picture an RNN as a loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "During  iteration $t$ of the loop\n",
    "- We consume input $\\x_\\tp$\n",
    "- Produce output $\\y_\\tp$ (which we will assume is the latent state: $\\y_\\tp = \\h_\\tp$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also indicated that we could \"unroll\" the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN unrolled</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_many_to_many.jpg\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer layer\n",
    "\n",
    "What would have happened if, rather than using the latent state approach, we choose the alternative:\n",
    "- Predict $\\y_\\tp$ as a direct function of the prefix of $\\x$ of length $\\tt$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the picture would look similar to the \"unrolled\" loop:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer layer</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_1.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compared to the unrolled RNN, the Transformer, the computation at step $t$\n",
    "- Has **no** data (e.g., $\\h_\\tp)$ passing from the computation between time steps (from $(t-1)$, to $(t+1)$)\n",
    "- Takes a **sequence** $\\x_{(1..t)}$ as input\n",
    "    - Because $\\y_\\tp$ is computed as a *direct* function of the prefix $\\x_{(1..t)}$ rather than recursively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In some instances, we may even allow the Transformer to \"see\" the *entire* input (not just a prefix) at each step $t$\n",
    "- The Encoder of an Encoder-Decoder architecture\n",
    "    - Context Sensitive Encoding\n",
    "        - Encode based on *entire* input\n",
    "        - Bi-directional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer layer</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_2.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Transformer uses *self-attention*\n",
    "- To influence which elements of $\\x_{(1 \\ldots \\, t)}$ to attend/focus to\n",
    "\n",
    "Looking inside the circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And there are cases where we *must not* allow the Transformer to \"see\" the *entire* input\n",
    "- The Decoder of an Encoder-Decoder architecture\n",
    "    - Teacher forcing: the input of step $(t+1)$ is $\\y_\\tp$, the output of step $t$\n",
    "    - Can't look ahead to something that has not yet been created !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For those cases where look-ahead is not allowed, the Transformer using **masking**\n",
    "- Hide the suffix $\\y_{(t+1 \\ldots \\, t)}$ from the attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Decoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You will notice two Attention layers\n",
    "- **Masked** Self Attention (on $\\y$)\n",
    "    - Allows the layer to focus on previous outputs\n",
    "        - Masked to prevent look-ahead to $\\y_{(t')}$ for $t' > t$\n",
    "- Encoder-Decoder Attention (on $\\bar\\h_\\tp$)\n",
    "    - Allows the Decoder to attend to the entire output sequence of the Encoder\n",
    "    \n",
    "So this layer attends to\n",
    "- previously generated Decoder layer outputs\n",
    "- the \"relevant\" part of the Encoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Transformer architecture just stacks $N$ Transformer layers.\n",
    "\n",
    "$N = 6$ was the choice of the original paper.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Stacked Transformer Layers (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_multi.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Full Encoder-Decoder Transformer architecture\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The diagram shows an Encoder-Decoder pair.\n",
    "\n",
    "You will notice that each element of the pair is different.\n",
    "\n",
    "- It is possible to use each element independently as well.\n",
    "\n",
    "- But first we need to understand\n",
    "the source of the differences and their implications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Encoder\n",
    "\n",
    "The Encoder side of the pair **does not** restrict the order in which it's inputs are accessed.\n",
    "- Self-attention **without** causal masking\n",
    "\n",
    "So the Encoder is appropriate for tasks that require a context-sensitive representation of\n",
    "each input element.\n",
    "\n",
    "For example: the meaning of the word \"**it**\" changes with a small change to a subsequent word in the following sentences:\n",
    "- \"The animal didn't cross the road because **it** was too tired\"\n",
    "\n",
    "- \"The animal didn't cross the road because **it** was too wide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some tasks with this characteristic are\n",
    "- Sentiment\n",
    "- Masked Language Modeling: fill-in the masked word\n",
    "- Semantic Search\n",
    "    - compare a summary of the sequence that is the context-sensitive representation of\n",
    "        - query sentence\n",
    "        - document sentences\n",
    "    - Each summary is a kind of **sentence embedding**\n",
    "    - Summary\n",
    "        - pooling over each word\n",
    "        - final token\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Decoder\n",
    "\n",
    "One notable aspect of the Decoder is its recurrent architecture\n",
    "- Output $\\y_{(\\tt-1)}$ is appended to the Decoder inputs available at step $\\tt$.\n",
    "    - The Decoder inputs are $\\y_{(1..T)}$, where $T$ is the full length of the Decoder output\n",
    "    - **But** Causal Masking ensures that only $\\y_{(1..\\tt)}$ is *available* at step $\\tt$.\n",
    "    \n",
    "Thus, the Decoder is appropriate for *generative* tasks\n",
    "- Text generation\n",
    "- Predict the next word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advantages of a Transformer compared to an RNN\n",
    "\n",
    "Among the most important advantages of the Transformer over an RNN\n",
    "- are its ability to capture long-term dependencies \n",
    "- because all elements of the sequence are processed in parallel\n",
    "    - no vanishing gradient or truncated back propagation\n",
    "    \n",
    "This has made the Transformer the architecture of choice for NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The computational advantages are many:\n",
    "- Time: All steps computed in parallel\n",
    "    - $O(1)$ sequential steps versus $O(T)$\n",
    "- Fewer operations: faster training\n",
    "    - $O( T^2 * d )$ versus $O(T * d^2)$, where $d$ is length of a single input element\n",
    "        - e.g., $\\x_\\tp$ replaced by an embedding of dimension $d$\n",
    "    - Transformer has fewer operations when $T \\lt d$\n",
    "- Similar number of parameters \n",
    "    - When $T < \\sqrt{d}$: Self attention has about the same number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that, because of TBTT, T is the length of a *chunk* rather than the full input length\n",
    "- Typical $T = 64, d \\ge 256$\n",
    "\n",
    "So under the special case (that applies to sequences) that chunk length is short relative to representation size,\n",
    "it is not \"crazy\" to perform all elements of $\\x$ with separate FC's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The faster training enables\n",
    "- larger datasets\n",
    "- deeper models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detailed computational comparison of architectures\n",
    "\n",
    "| Type | Parameters  | Operations\\;\\; | Path length |\n",
    "|------|------       |------      |------       |\n",
    "|  CNN | $k * d^2$   | $T * k * d^2$ | $T$ |\n",
    "| RNN  | $d^2$       | $T * d^2$     | $T$ |\n",
    "| Self-attention | $T^2 *d $ | $T^2 *d$ | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the details of the math        \n",
    "\n",
    "Attention involves a dot product (of vectors of length $d$)\n",
    "- Each input matched against all others: $T * T$\n",
    "- So $T^2 *d$ operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN\n",
    "- $T$ sequential steps\n",
    "- Each step evaluates\n",
    "    $$\n",
    "\\h_\\tp  =  \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \n",
    "$$\n",
    "- $\\h_\\tp$ has multiple elements, assume $|| \\h || = O(d)$\n",
    "    - Computing updated hidden state element $j$ (i.e., $\\h_{\\tp, j}$) involves dot product of vectors of length $d$ (size of $\\x_\\tp)$\n",
    "    - $d$ multiplications per element of $\\h$, times $O(d)$ elements of $\\h$ is $O(d^2)$ per step\n",
    "    - So $T * d^2$ operations\n",
    "    \n",
    "- $\\W_{hh}$ matrix: $d^2$ parameters\n",
    "  - $ | \\h | = d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "CNN\n",
    "- path length $T$ \n",
    "    - each kernel multiplication connects only $k$ elements of $\\x$\n",
    "    - since kernels overlap inputs, can't parallelize, hence $O(T/k)$ path length\n",
    "        - can reduce to $\\log(T)$ with tree structure\n",
    "- Parameters\n",
    "    - kernel size $k$\n",
    "    - number of input channels = number of output channels = $d$\n",
    "    - $k *d$ parameters for kernel of one channel\n",
    "    - $k * d^2$ parameters for kernel for all $d$ output channels\n",
    "    \n",
    "- Operations\n",
    "    - for a single output channel: $k$ per input channel\n",
    "        - There are $d$ input channels, so $k *d$ for each dot product of *one* output channel\n",
    "        - There are $d$ output channels, so $k * d^2$ per time step\n",
    "    - $T$ time steps so $T * k * d^2$ number of operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN\n",
    "- $\\W_{hh}$ matrix: $d^2$ parameters\n",
    "  - $ | \\h | = d$\n",
    "- $T * d^2$ operations (for entire sequence)\n",
    "- path length $T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize\n",
    "- for short chunk/sequence length, relative to size of hidden state\n",
    "    - $|x| \\lt 64$ typically; $d \\approx 256$\n",
    "- Transformer/self attention is comparable in terms of number of parameters\n",
    "\n",
    "So under the special case (that applies to sequences) that chunk length is short relative to representation size,\n",
    "it is not \"crazy\" to perform all elements of $\\x$ with separate FC's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A free lunch ? Almost !\n",
    "\n",
    "Transformers offer the possibility of great improvements in training speed\n",
    "- Parallelism\n",
    "- Fewer operations\n",
    "    \n",
    "Sounds too good to be true.  Is there such a thing as a free lunch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Almost\n",
    "- RNN can handle sequences of arbitrary length ($T$ unbounded)\n",
    "- Transformer has a fixed number of parallel units, which limits the length of sequences\n",
    "\n",
    "But, in practice: RNN uses *Truncated* Back Propagation Through Time\n",
    "- So the maximum distance between input sequence elements is bounded by $k$, the truncation length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some other advantages\n",
    "\n",
    "- Can learn long-range dependencies\n",
    "    - Gradients within a layer don't flow backwards: always a single step\n",
    "        - Can't vanish or explode\n",
    "    - The output $\\y^{[\\ll]}_\\tp$ of layer $\\ll$ (for stacked Transformer layers) is a function of **all** inputs\n",
    "    $$\n",
    "    \\y^{[\\ll-1]}_{(\\tt')} \\text{ for } 1 \\le \\tt' \\le T\n",
    "    $$\n",
    "        - so can directly access a distant input\n",
    "        - not diminished by passing through multiple intermediate time steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some drawbacks\n",
    "\n",
    "- The output $\\y^{[\\ll]}_\\tp$ of layer $\\ll$ (for stacked Transformer layers) is a function of **all** inputs, **always**\n",
    "    - Perhaps less efficient\n",
    "- Unless you add positional encoding, you lose ordering relationships between inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
