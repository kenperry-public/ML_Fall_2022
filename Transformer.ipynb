{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with Sequences: Recurrent Neural Network (RNN) layer\n",
    "\n",
    "For a function that takes \n",
    "sequence $\\x^\\ip$ as input\n",
    "and creates sequence $\\y$ as  output we had two choices for implementing the function.\n",
    "\n",
    "The RNN implements the function as a \"loop\"\n",
    "- A function that taking **a single** $\\x_\\tp$ as input a time\n",
    "- Outputting $\\y_\\tp$ \n",
    "- Using a \"latent state\" $\\h_\\tp$  to summarize the prefix $\\x_{(1\\ldots \\tt)}$\n",
    "- Repeat in a loop over $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\h_\\tp | \\x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } [ \\x_{(1)} \\dots \\x_\\tp ]\\\\\n",
    "\\pr{\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "    \n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Loop with latent state</strong></center>\n",
    "    <img src=\"images/RNN_arch_loop.png\" width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Unrolling\" the loop makes it equivalent to a multi-layer network\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN unrolled</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_many_to_many.jpg\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer: Encoder-style\n",
    "\n",
    "The alternative to the loop was to create a \"direct function\"\n",
    "- Taking a **sequence** $\\x_{(1 \\dots \\tt)}$ as input\n",
    "- Outputting $\\y_\\tp$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function</strong></center>\n",
    "    <img src=\"images/RNN_arch_parallel.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to output the sequence $\\y_{(1)} \\ldots \\y_{(T)}$ we\n",
    "create $T$ copies of the function (one for each $\\y_\\tp$)\n",
    "- computes each $\\y_\\tp$ in **parallel**, not sequentially as in the loop\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function, in parallel (masked input)</strong></center>\n",
    "<img src=\"images/Transformer_parallel_masked.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The parallel units constitute a *Transformer Encoder*\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Encoder (causal masked input)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_1.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compared to the unrolled RNN, the Transformer Decoder\n",
    "- Takes a **sequence** $\\x_{(1..t)}$ as input\n",
    "    - Because $\\y_\\tp$ is computed as a *direct* function of the prefix $\\x_{(1..t)}$ rather than recursively\n",
    "- Has **no** latent state: output is a direct function of the input sequence\n",
    "- Has **no** data (e.g., $\\h_\\tp)$ passing from the computation between time steps (e.g., from $\\tt$ to $(\\tt +1)$)\n",
    "- Outputs generated in parallel, not sequentially\n",
    "- No gradients flowing backward over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this architecture, we can compute more general functions than the RNN\n",
    "- where each $\\y_\\tp$ depends on the entire $\\x_{(1 \\ldots T)}$ rather than a prefix $\\x_{(1 \\ldots \\tt)}$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function, in parallel (un-masked input)</strong></center>\n",
    "<img src=\"images/Transformer_parallel.png\" width=50%>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Decoder (unmasked input)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_2.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example of such a general function is the \"meaning\" of a word, in context.\n",
    "\n",
    "| Sentence | Meaning of \"it\" |\n",
    "|:----------|:-----------------|\n",
    "The animal didn't cross the street because **it** was too tired | the animal\n",
    "The animal didn't cross the street because **it** was too wide  | the road\n",
    "\n",
    "The meaning of the word \"it\" is determined by a word that follows it (\"tired\" or \"wide\")\n",
    "\n",
    "So even though the Transformer output at each position is a function of the entire sequence $\\x_{(1 \\ldots T)}$\n",
    "- the output is different for each position $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can control whether the input to Transformer element at position $\\tt$ is\n",
    "prefix $\\x_{(1 \\ldots \\tt)}$\n",
    "or\n",
    "the entire sequence $\\x_{(1 \\ldots T)}$\n",
    "by **masking** the input to element $\\tt$\n",
    "- no masking: the entire sequence is visible\n",
    "- *casual masking*: only the prefix up to $\\tt$ is visible: $\\x_{(1 \\ldots \\tt)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Technical clarifications\n",
    "\n",
    "When we introduced the RNN, at each step $\\tt$ of the loop, we defined two outputs\n",
    "$\\h_\\tp$\n",
    "and\n",
    "$\\y_\\tp$.\n",
    "\n",
    "In general, we only need to output $\\h_\\tp$\n",
    "- $\\y_\\tp$ can be defined as a further processing of $\\h_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Henceforth, we will assume the style of a single output $\\h_\\tp$\n",
    "\n",
    "The reason for doing this:\n",
    "- We can \"stack\" $N$ Transformer layers (just as we can stack RNN layers)\n",
    "- The output of the non-top layer $j$ is $\\h^{[j]}_\\tp$, not the final $\\y_\\tp$\n",
    "- We identify $\\y_\\tp$ as the output of the top layer $\\h^{[N]}_\\tp$\n",
    "    - perhaps after a further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Furthermore: \n",
    "    \n",
    "Since the Encoder part is no longer a \"loop\"\n",
    "- It is inaccurate to refer to the Encoder output $\\bar \\h_\\tp$ as a \"latent\" state\n",
    "- However, $\\bar \\h_\\tp$ *is still* a summary of the input sequence\n",
    "    - a summary of $\\x_{(1 \\ldots \\tt)}$ when casual attention is used\n",
    "    - a summary of $\\x_{(1 \\ldots \\bar T)}$ otherwise\n",
    "- Out of **bad habit** we may continue to erroneously refer to $\\bar \\h$ and $\\h$ as \"latent\" states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the Transformer Encoder: Self Attention\n",
    "\n",
    "If we look inside the box computing the direct function, we will find several layers\n",
    "- An Attention layer\n",
    "    - To influence which elements of the input sequence $\\x$ to attend/focus when outputting $\\y_\\tp$\n",
    "- A Feed Forward Network (FF) layer to compute the function\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An Attention layer that attends/focus on its inputs implements what is called *Self-attention*\n",
    "- We will soon see the possibility of attending to other values\n",
    "\n",
    "If the function for $\\h_\\tp$ is restricted to prefix $\\x_{(1 \\ldots \\tt)}$ the Attention layer can use causal masking of the input sequence.\n",
    "\n",
    "This is referred to as *Masked Self-Attention*.\n",
    "\n",
    "The Feed Forward Network computes the function, given the elements of the input sequence\n",
    "that are being attend to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advantages of a Transformer compared to an RNN\n",
    "\n",
    "As we will demonstrate in detail below\n",
    "- The Transformer's operations can be performed in parallel versus sequentially for the RNN\n",
    "- Gradients less likely to vanish or explode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can **leverage** these advantages in complexity by\n",
    "- By making a Transformer model bigger (e.g., more stacked Transformer layers)\n",
    "- Making the sequence lengths longer\n",
    "- Increasing the number of examples of training data\n",
    "\n",
    "So, for the same time \"cost\" as an RNN, we can use a bigger Transformer on more data\n",
    "- **Hence: we can learn more complex functions for similar time cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover: the path length from the output to the input is constant in an Transformer, compared to $T$ in the RNN.\n",
    "\n",
    "Recall that gradients tend to vanish or explode as the path length during Back Propagation increases.\n",
    "\n",
    "So **Transformers are better able to capture long-range dependencies than an RNN**\n",
    "\n",
    "This gives them an advantage in learning as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The price we pay for this is that the number of parameters of a Transformer is greater than a similar RNN\n",
    "- By a factor of $T$\n",
    "- The parameters for each time step of a Transformer are *independent*\n",
    "- The parameters for each time step of an RNN are *shared*\n",
    "\n",
    "We give the detailed math below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of sequential steps\n",
    "\n",
    "The most obvious advantage of the \"direct function\" as opposed to the \"loop\" is\n",
    "that outputs are computed in parallel versus sequentially.\n",
    "\n",
    "For an input sequence of length $T$:\n",
    "- The loop requires $T$ steps\n",
    "- The direct function requires $1$ step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Path length\n",
    "The *Path length* is the distance that the Loss Gradient needs to travel backwards during Back Propagation.\n",
    "\n",
    "At each step, the gradient is subject to being diminished or increased (Vanishing/Exploding gradients).\n",
    "\n",
    "Since the Transformer operates in parallel across positions, this is $\\OrderOf{1}$.\n",
    "\n",
    "It is $\\OrderOf{T}$ for the RNN due to the sequential computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The constant path length is critical to the success of the Transformer**\n",
    "- The query used for the input at position $\\tt$ can access **all** prior positions $\\tt' \\le \\tt$ at the same cost\n",
    "    - Gradient not diminished\n",
    "    - RNN\n",
    "        - Gradient signal diminished for position $\\tt' << \\tt$\n",
    "        - Truncated Back Propagation may kill the gradient flow from position $\\tt$ back to $\\tt'$ beyond truncation window\n",
    "\n",
    "A key strength of the Transformer is that it enables learning long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of operations\n",
    "\n",
    "What about the number of operations ? \n",
    "\n",
    "Let $d$ denote the length of the output of a Transformer\n",
    "- i.e., $d = || \\h_\\tp ||$\n",
    "\n",
    "When we examine the internals of the Transformer in precise detail\n",
    "- We will discover additional layers\n",
    "- The size of the output of each layer is also $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Self Attention layer attend to (transformed) inputs\n",
    "- each element assumed size of $d$\n",
    "\n",
    "The keys and values of the CSM implementing Attention are the size $d$ input elements.\n",
    "- Each attention lookup (dot product of query with a key) requires $d$ multiplications.\n",
    "- There are $T$ key/value pairs in the CSM\n",
    "- There are $T$ attention units (one for each position, outputting $\\h_\\tp$)\n",
    "\n",
    "Thus: $\\OrderOf{T^2 *d}$ multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What about the number of operations for an RNN computing the same function ?\n",
    "\n",
    "The RNN outputs $\\h_\\tp$  of size $d$ (same as Transformer).\n",
    "- In the RNN $\\h_\\tp$ is also the latent state\n",
    "\n",
    "The RNN \"loops\" for $T$ steps.\n",
    "\n",
    "Each step updates latent state $\\h_\\tp$ via the equation\n",
    "    $$\n",
    "\\h_\\tp  =  \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \n",
    "$$\n",
    "- $\\x_\\tp$ is also size $d$ (same assumption as for Transformer)\n",
    "- The  weight matrices \n",
    "$$\\W_{xh} \\text{ and } \\W_{hh}$$\n",
    "are of size $$\\OrderOf{d \\times d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So each step involves $d^2$ multiplications.\n",
    "\n",
    "For $T$ steps: \n",
    " $\\OrderOf{T * d^2}$ multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transformer number of operations: $\\OrderOf{T^2 * d}$\n",
    "\n",
    "RNN number of operations $\\OrderOf{T * d^2}$\n",
    "\n",
    "When $T \\lt d$, the Transformer uses fewer operations compared to the RNN.\n",
    "\n",
    "Typical values\n",
    "- $d \\ge 768$\n",
    "- $T \\lt d$ in typical RNN\n",
    "    - remember: TBPTT divides the input sequence into shorter segments\n",
    "- **but** $T > d$ in the most recent Transformer modeles\n",
    "    - Path length is constant, so able to increase $T$ without fear of vanishing/exploding gradients\n",
    "    - Can capture very long-term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of parameters\n",
    "\n",
    "Multi-head Attention transforms the keys and values of the CSM\n",
    "from length $d$ to a fraction (determined by number of heads) of $d$.\n",
    "\n",
    "So these matrices are of size $\\OrderOf{d^2}$.\n",
    "\n",
    "The matrices for each position $\\tt$ are not shared.\n",
    "\n",
    "With $T$ positions, the total number of parameters is $\\OrderOf{T * d^2}$.\n",
    "\n",
    "**Note**\n",
    "\n",
    "With multi-head attention, $d_\\text{head} = \\frac{d}{n_\\text{heads}}$\n",
    "\n",
    "The matrices are thus of size $d_\\text{head}^2$; there are $n_\\text{heads}$ of these matrices for\n",
    "total size $n_\\text{heads} * d_\\text{head}^2 = \\frac{d^2}{n_\\text{heads}}$\n",
    "\n",
    "This is $\\OrderOf{d^2}$ but with a smaller multiplicative constant\n",
    "- Can explain the difference when trying to perform an exact calculation of number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The FFN layer (assuming a single Fully Connected layer) would also have $d^2$ parameters.\n",
    "- These **are shared** across positions.\n",
    "- So only $\\OrderOf{d^2}$ parameters for the shared FFN.\n",
    "\n",
    "Total number of parameters in the combined Attention + FFN layers: $\\OrderOf{T * d^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We previously derived that the size of the weight matrices in the RNN are $\\OrderOf{d^2}$.\n",
    "\n",
    "The number of parameters in the Transformer are $\\OrderOf{d^2}$.\n",
    "\n",
    "So the number of parameters in the Transformer is larger by a factor of $T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complexity: summary\n",
    "\n",
    "We also throw in a CNN for comparison\n",
    "\n",
    "The detailed CNN math is given in a following section.\n",
    "\n",
    "| Type | Parameters  | Operations  &nbsp; &nbsp; &nbsp; | Sequential steps | Path length\n",
    "|:------|:---|:---|:---|:---|\n",
    "|  CNN | $\\OrderOf{k * d^2}$   | $\\OrderOf{T * k * d^2}$ | $\\OrderOf{T}$   | $\\OrderOf{T}$ |\n",
    "| RNN  | $\\OrderOf{d^2}$       | $\\OrderOf{T * d^2}$     | $\\OrderOf{T}$    | $\\OrderOf{T}$ |\n",
    "| Self-attention | $\\OrderOf{T *d^2} $ | $\\OrderOf{T^2 *d}$ | $\\OrderOf{1}$ | $\\OrderOf{1}$ |\n",
    "\n",
    "Reference:\n",
    "- [Table 1 of Attention paper](https://arxiv.org/pdf/1706.03762.pdf#page=6)\n",
    "- See [Stack overflow](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model) for correction of the number Operations calculated in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the details of the math for the CNN\n",
    "\n",
    "- path length $T$ \n",
    "    - each kernel multiplication connects only $k$ elements of $\\x$\n",
    "    - since kernels overlap inputs, can't parallelize, hence $\\OrderOf{T/k}$ path length\n",
    "        - can reduce to $\\log(T)$ with tree structure\n",
    "- Parameters\n",
    "    - kernel size $k$\n",
    "    - number of input channels = number of output channels = $d$\n",
    "    - $k *d$ parameters for kernel of one channel\n",
    "    - $\\OrderOf{k * d^2}$ parameters for kernel for all $d$ output channels\n",
    "    \n",
    "- Operations\n",
    "    - for a single output channel: $k$ per input channel\n",
    "        - There are $d$ input channels, so $k *d$ for each dot product of *one* output channel\n",
    "        - There are $d$ output channels, so $k * d^2$ per time step\n",
    "    - $T$ time steps so $\\OrderOf{T * k * d^2}$ number of operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A free lunch ? Almost !\n",
    "\n",
    "Transformers sound almost too good to be true\n",
    "- Faster compute (through reduced number of Sequential steps)\n",
    "- Constant Path Length\n",
    "    - Better able to capture long range dependencies\n",
    "    \n",
    "Is there really such a thing as a free lunch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Almost.\n",
    "\n",
    "The Transformer is potentially more expensive in some aspects\n",
    "- more weights\n",
    "- more operations (but the operations for each of the $T$ positions can occur in parallel)\n",
    "\n",
    "Moreover: the RNN can handle sequences of arbitrary length ($T$ unbounded)\n",
    "- Transformer has a fixed number of parallel units, which limits the length of sequences\n",
    "\n",
    "But, in practice: RNN uses *Truncated* Back Propagation Through Time\n",
    "- So the maximum distance between input sequence elements is bounded by $k$, the truncation length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some drawbacks\n",
    "\n",
    "- The output $\\y^{[\\ll]}_\\tp$ of layer $\\ll$ (for stacked Transformer layers) is a function of **all** inputs, **always**\n",
    "    - Perhaps less efficient\n",
    "- Unless you add positional encoding, you lose ordering relationships between inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer: Decoder style\n",
    "\n",
    "It is common to use two Transformers in an Encoder-Decoder configuration.\n",
    "\n",
    "Recall the Encoder-Decoder architecture (using RNN's rather than Transformers in the diagram)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>Encoder-Decoder for language translation</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Language_Translation.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder in the Encoder-Decoder architecture is *generative*\n",
    "- Outputs $\\hat \\y_\\tp$ for a single $\\tt$ at a time\n",
    "- Appending output $\\hat \\y_\\tp$ to the input available to output the next $\\hat \\y_{(\\tt+1)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder in the Encoder-Decoder architecture creates a latent state $\\bar \\h_\\tp$ which summarizes the input prefix $\\x_{(1 \\ldots \\tt)}$.\n",
    "\n",
    "In the above diagram the Decoder only has access to $\\bar\\h_{(\\bar T)}$,\n",
    "the final latent state\n",
    "- summarizing the entire input sequence\n",
    "\n",
    "This is very restrictive, forcing $\\bar \\h_{(\\bar T)}$ to encode a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we motivated Attention by suggesting that the Decoder have access to *each* $\\bar \\h_\\tp$ for $1 \\le \\tt \\le \\bar T$.\n",
    "- and use the Attention mechanism to decide which $\\bar \\h_\\tp$ to focus on when generating $\\hat \\y_\\tp$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder: Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Decoder Transformer*\n",
    "- is similar to the Encoder Transformer in that both use Self-Attention to their own inputs\n",
    "- differs in that it can also attend to the output of the Encoder.\n",
    "\n",
    "Attending to the output of another model (e.g., Decoder attending to Encoder output) is called *Cross Attention* (Encoder-Decoder Attention).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Decoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The combinded Encoder-Decoder Transformer diagram looks like this\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_2.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Explanation of diagram**\n",
    "- The Encoder uses Self-attention (<span style=\"color:green\">wide Green arrow</span>) to attend to input sequence $\\x$\n",
    "- The Decoder uses Masked Self-attention (<span style=\"color:red\">wide Red arrow</span>) to attend to its input\n",
    "    - It's input is the prefix of the output sequence $\\y$\n",
    "    - Limited to prefix of length $\\tt$ by **masking**\n",
    "- The Decoder uses Cross Attention (between Encoder and Decoder) <span style=\"color:blue\">(wide Blue arrow)</span>\n",
    "    - To enable Decoder to focus on which Encoder latent state $\\bar \\h_\\tp$ to atttend to\n",
    "- The dotted <span style=\"color:blue\">(thin Blue arrow)</span> indicates that the output $\\hat \\y_\\tp$ is appended to the input that is available when generating $\\hat \\y_{(\\tt+1)}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the Decoder is recurrent (generative)\n",
    "- it generates a single output at a time\n",
    "- unlike the Encoder, which generates all outputs (i.e., \"encodings\") in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Functional versus Sequential architecture\n",
    "\n",
    "The architecture diagram is more complex than we have seen thus far.\n",
    "\n",
    "In particular: data no longer strictly flows forward in a layer-wise arrangement !\n",
    "- There are two independent sub-networks (Encoder and Decoder)\n",
    "- Connection from the Encoder output to the middle of the Decoder (Cross-Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each of the Encoder and Decoder is an independent Functional model.\n",
    "- not our familiar Sequential modles\n",
    "\n",
    "The Encoder-Decoder pair combination is also constructed as a Functional model.\n",
    "\n",
    "Since we have not yet addressed Functional Models, you may not be prepared to completely grasp the totality.\n",
    "\n",
    "But hopefully you can absorb the concepts even without fully understanding the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Detailed Encoder-Decoder Transformer architecture\n",
    "\n",
    "There are other components of the Encoder and Decoder that we have yet to describe.\n",
    "\n",
    "We will do so briefly.\n",
    "\n",
    "(The Transformer was introduced in the paper [Attention is all you Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "   \n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Embedding layers**\n",
    "\n",
    "We will motivate and describe Embeddings in the NLP module.\n",
    "\n",
    "For now:\n",
    "- an embedding is an encoding of a categorical value that is shorter than OHE\n",
    "\n",
    "It is used in the Transformer to\n",
    "- encode the input sequence of words\n",
    "- encode the output sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Positional Encoding**\n",
    "\n",
    "The inputs are ordered (i.e., sequences) and thus describe a relative ordering relationship between elements.\n",
    "\n",
    "But inputs to most layer types (e.g., Fully Connected) are unordered.\n",
    "\n",
    "The Positional Encoding is a way of encoding the the relative ordering of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To represent the relative position of each element in the sequence,\n",
    "- we can pair the input element  with an encoding of its position in the sequence.\n",
    "$$\n",
    "\\langle \\x_\\tp, \\text{encode}(\\tt) \\rangle\n",
    "$$\n",
    "\n",
    "The box labeled \"Positional Encoding\" creates $\\text{encode}(\\tt)$.\n",
    "\n",
    "The \"+\" concatenates the Input Embedding and Positional Encoding to create $\n",
    "\\langle \\x_\\tp, \\text{encode}(\\tt) \\rangle\n",
    "$.\n",
    "\n",
    "If relative position is important, the NN can learn the values of  $\\text{encode}(\\tt)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Self Attention layers (Encoder and Decoder)**\n",
    "\n",
    "The 3 arrows flowing into the Multi-Head Attention box\n",
    "- are identical\n",
    "- are the inputs (after being Embedded and having Positional Encoding added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Self-Attention layers for the Encoder and Decoder\n",
    "- **differ in that the Decoder uses Causal Masking versus no-masking for the Encoder**\n",
    "- Decoder can't \"look ahead\" at output $\\y_{\\tt'}$ for $\\tt' \\ge \\tt$\n",
    "    - it hasn't been generated yet at test time step $\\tt$\n",
    "    - it **is** available at training time (via Teacher Forcing)\n",
    "        - but shouldn't look at it during training time, in order for training to be similar to test time\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cross Attention layer (Decoder)**\n",
    "\n",
    "The two arrows flowing from the Encoder output are the keys and values of the CSM\n",
    "\n",
    "The arrow flowing from the Self Attention layer is the query\n",
    "- The output of the Self Attention layer is the **query** used in Cross Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Add and Norm**\n",
    "\n",
    "We have seen each of these layer types before\n",
    "- Norm: Batch (or other) Normalization layers\n",
    "- Add: the part of the residual network that joins outputs of multiple previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The diagram shows an Encoder-Decoder pair.\n",
    "\n",
    "You will notice that each element of the pair is different.\n",
    "\n",
    "- It is possible to use each element independently as well.\n",
    "\n",
    "- But first we need to understand\n",
    "the source of the differences and their implications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How is the direct function computed ?\n",
    "\n",
    "The Encoder uses self-attention\n",
    "- So the keys and values of the CSM are derived directly from input sequence $x_{(1 \\ldots T)}$\n",
    "\n",
    "During training, the Encoder\n",
    "- learns a query, derived from input sequence $\\x_{(1 \\ldots T)}$\n",
    "- learns weights for the Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Attention output \n",
    "- is equal to a weighted combination of CSM values\n",
    "    - i.e., weighted sum of input elements\n",
    "\n",
    "The Feed Forward Network transforms the Attention output into Encoder output $\\bar \\h_\\tp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly for the Decoder.\n",
    "\n",
    "The Self-Attention layer CSM  has keys and values that are incrementally constructed\n",
    "from the outputs $\\y_{(1 \\ldots, \\tt)}$ that have been created from the first $\\tt$ steps.\n",
    "\n",
    "The Cross-Attention layer CSM has keys and values that are outputs $\\bar \\h_\\tp$ of the Encoder.\n",
    "\n",
    "During training, the Self-Attention layer outputs **the query** that is used for Cross Attention.\n",
    "\n",
    "The query is created by self-attention to the inputs.\n",
    "\n",
    "The Decoder **learns (from training)** \n",
    "- the Self-Attention query \n",
    "- the Cross Attention query\n",
    "- the weights of the Feed Forward Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacked Transformer\n",
    "\n",
    "Just as with many other layer types (e.g., RNN), we may stack Transformer layers.\n",
    "- Each layer creating alternate representations of the input of increasing complexity\n",
    "\n",
    "In fact, stacking $N > 1$  Transformer layers is typical.\n",
    "\n",
    "$N = 6$ was the choice of the original paper.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Stacked Transformer Layers (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_multi.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uses of  an Encoder-style Transformer\n",
    "\n",
    "The Transformer for the Encoder and Decoder of an Encoder-Decoder Transformer are slightly different.\n",
    "\n",
    "They can also be used individually as well as in pairs.\n",
    "\n",
    "It's important to understand the differences in order to know when to use each individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder side of the pair **does not** restrict the order in which it's inputs are accessed.\n",
    "- Self-attention **without** causal masking\n",
    "\n",
    "So the Encoder is appropriate for tasks that require a context-sensitive representation of\n",
    "each input element.\n",
    "\n",
    "For example: the meaning of the word \"**it**\" changes with a small change to a subsequent word in the following sentences:\n",
    "- \"The animal didn't cross the road because **it** was too tired\"\n",
    "\n",
    "- \"The animal didn't cross the road because **it** was too wide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some tasks with this characteristic are\n",
    "- Sentiment\n",
    "- Masked Language Modeling: fill-in the masked word\n",
    "- Semantic Search\n",
    "    - compare a summary of the sequence that is the context-sensitive representation of\n",
    "        - query sentence\n",
    "        - document sentences\n",
    "    - Each summary is a kind of **sentence embedding**\n",
    "    - Summary\n",
    "        - pooling over each word\n",
    "        - final token\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uses of a Decoder-style Transformer\n",
    "\n",
    "One notable aspect of the Decoder is its recurrent (generative) architecture\n",
    "- Output $\\y_{(\\tt-1)}$ is appended to the Decoder inputs available at step $\\tt$.\n",
    "    - The Decoder inputs are $\\y_{(1..T)}$, where $T$ is the full length of the Decoder output\n",
    "    - **But** Causal Masking ensures that only $\\y_{(1..\\tt)}$ is *available* at step $\\tt$.\n",
    "    \n",
    "Thus, the Decoder is appropriate for *generative* tasks\n",
    "- Text generation\n",
    "- Predict the next word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The Transformer architecture has come to dominate tasks with long sequences (e.g., NLP).\n",
    "\n",
    "The operations of a Transformer occur in parallel for each position.\n",
    "\n",
    "This allows us to leverage the compute time\n",
    "- Use many stacked Transformer layers\n",
    "- At time cost still less than a sequential RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, the constant path length means the gradients are less likely to vanish/explode for long sequences\n",
    "- No need to truncate Back Propagation as in an RNN\n",
    "- Long term dependencies between positions become feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We pay for these advantages in terms of increasing\n",
    "- number of operations\n",
    "    - but they occur in parallel, so no increase in elapsed time\n",
    "- number of weights\n",
    "\n",
    "Thus, Transformer training is both compute and memory intensive.\n",
    "- This limits the number of individuals/organizations able to train very large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
