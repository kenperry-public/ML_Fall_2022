{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\o}{\\mathbf{o}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced datasets\n",
    "\n",
    "What happens when our *training examples* are imbalanced\n",
    "- some examples over-represented\n",
    "- other examples under-represented\n",
    "\n",
    "We already briefly covered this in [Loss Analysis](Training_Loss.ipynb#Conditional-loss)\n",
    "- Our motivation there was focusing on examples where errors occur\n",
    "- Here our motivation is when the examples naturally partition into *imbalanced* subsets\n",
    "\n",
    "\n",
    "We revisit this in the case of a Binary Classification task, where one class dominates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training loss is usually given unconditionally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis.png\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "â€¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can also partition the examples and examine the loss in each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Loss analysis: conditional loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis_1.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we partition the training examples into those whose class is Positive and those whose class is Negative:\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\langle \\X, \\y \\rangle & = & [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ] \\\\\n",
    " & = & [ \\x^\\ip, \\text{Positive} | 1 \\le i \\le m' ] \\; \\cup \\; [ \\x^\\ip, \\text{Negative} | 1 \\le i \\le m'' ] \\\\ \n",
    " & = & \\langle \\X_{(\\text{Positive})}, \\y_{(\\text{Positive})} \\rangle \\; \\cup \\; \\langle \\X_{(\\text{Negative})}, \\y_{(\\text{Negative})} \\rangle\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can partition the training loss\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\\n",
    "& = & \\frac{m'}{m} { 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\; \\frac{m''}{m} { 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "That is, the Average loss is the weighted (with weights $\\frac{m'}{m}, \\frac{m''}{m}$) conditional losses\n",
    "- ${ 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta$\n",
    "-  ${ 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we've observed before\n",
    "- As long as the majority class dominates in count (e.g., $m' \\gg m''$)\n",
    "- It is possible for Average Loss to be low\n",
    "- Even if Conditional Loss for the minority class is high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that training is less likely to generalize well out of sample to the minority examples.\n",
    "\n",
    "When the set of training examples $\\langle \\X, \\y \\rangle$ is such that\n",
    "- $\\y$ comes from set of categories $C$\n",
    "- Where the distribution of $c \\in C$ is *not* uniform\n",
    "\n",
    "the dataset is called *imbalanced*.\n",
    "\n",
    "This means that training is may be biased to not do as well on examples from under-represented classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the Titanic survival:\n",
    "- only 38% of the passengers survived, so the dataset is highly imbalanced\n",
    "- a naive model that *always* predicted \"Not survived\" will\n",
    "    - have 62% accuracy\n",
    "        - be correct 100% of the time for 62% of the sample (those that didn't survive)\n",
    "        - be incorrect 100% of the time for 38% of the sample (those that did survive)\n",
    "\n",
    "The question is whether your use case requires high accuracy in *all* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approaches to imbalanced training data\n",
    "\n",
    "There are a number of approaches to avoid a potential bias caused by imbalanced data.\n",
    "\n",
    "[The `imbalanced-learn` website](https://imbalanced-learn.org/stable/user_guide.html) documents approaches to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Loss\n",
    "- Use conditional metrics rather than unconditional metrics\n",
    "    - Metric less influenced by size\n",
    "    - Combination of Precision and Recall\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose a model that is not sensitive to imbalance\n",
    "- Decision Trees\n",
    "    - branching structure can handle imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss sensitive training\n",
    "\n",
    "Modify the Loss function to weight conditional probabilities\n",
    "\n",
    "Rather than\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\\n",
    "& = & \\frac{m'}{m} { 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\;  \\frac{m''}{m} { 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "adjust weights\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & \n",
    "C_{\\text{Positive}} * \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\; C_{\\text{Negative}} * \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Equally weighted across classes: $C_\\text{Positive} = C_\\text{Negative}$\n",
    "- Relative importance\n",
    "    - An error in one class may be more important than an error in the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most models in `sklearn` take an optional `class_weights` optional argument\n",
    "\n",
    "Allows for different importance (w.r.t. Loss) of different classes\n",
    "- Can be used to address imbalance\n",
    "- Can be used to address increased importance (regardless of size) of a particular class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "We will describe several methods for modifying the training examples.\n",
    "\n",
    "They usually involve adding examples in order to achieve balance across classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling\n",
    "\n",
    "Re-sampling is a process of constructing a new set of training examples by\n",
    "drawing samples from the original.\n",
    "\n",
    "The sampling doesn't have to be uniform and may be used such that the resampled examples are more balanced\n",
    "- We can oversample (draw with higher probability) the Minority class\n",
    "- We can undersample (draw with lower probability) the Majority class\n",
    "\n",
    "In the limit, weighted sampling is almost equivalent to using the original examples, but\n",
    "weighting the loss term for each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Imbalanced data: Oversample the minority</strong></center>\n",
    "    </tr>\n",
    "<img src=images/Oversample.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Imbalanced data: Undersample the majority</strong></center>\n",
    "    </tr>\n",
    "<img src=images/Undersample.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Synthetic examples\n",
    "\n",
    "Oversampling merely duplicates existing examples from the Minority class.\n",
    "\n",
    "There are techniques to generate *synthetic* examples\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A rough description of creating a synthetic example\n",
    "- Choose an example; find \"close\" neighboring examples of the same class (Minority)\n",
    "    - Using a metric of distance between examples\n",
    "- Create an example that blends/interpolates features from the neighbors into a new example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues with augmenting the training examples\n",
    "\n",
    "We have described several techniques for augmenting the training examples.\n",
    "- The goal is to make the Loss calculation more sensitive to the under-represented class\n",
    "\n",
    "We could achieve the same effect by using a weighted Loss function (Loss sensitive training)\n",
    "- Setting $C_\\text{Positive} = C_\\text{Negative}$ will give equal weight to both classes, regardless of the size of each\n",
    "\n",
    "From a mathematical perspective (w.r.t., Loss) these are equivalent.\n",
    "\n",
    "**But** there are important practical differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The augmented set of examples used for training\n",
    "- has been made *different* in distribution (i.e., balanced)\n",
    "- from the actual out of sample examples that will be experienced post-training (imbalanced)\n",
    "\n",
    "But if we just feed the augmented examples into cross-validation\n",
    "- the held-out fold will come from the augmented distribution, not the true distribution\n",
    "\n",
    "(The same would happen if we just used a fraction of the augmented examples as a Validation set.)\n",
    "\n",
    "Thus, the metrics computed by cross-validation on the hold-out folds will not be predictive of true out of sample behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Any* calculation (e.g., conditional metrics)\n",
    "- using a subset of the augmented examples as a proxy for out of sample \n",
    "- will not be representative of true out of sample\n",
    "\n",
    "We must keep this in mind when using Augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, from a practical perspective\n",
    "- Modifying the Loss function (loss-sensitive training)\n",
    "- May avoid \"side-effects\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced example\n",
    "Here's an unbalanced dataset with 2 classes, and the associated predictions of some model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([0, 1, 0, 0, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the regular and class balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acccuracy=0.667\n",
      "Class balanced Acccuracy=0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Acccuracy={a:3.3f}\".format(a=accuracy_score(y_true,  y_pred)))\n",
    "print(\"Class balanced Acccuracy={a:3.3f}\".format(a=balanced_accuracy_score(y_true,  y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the math behind the two accuracy computations\n",
    "- \"Regular accuracy\": per class conditional accuracy, weight by class fraction as percent of  total\n",
    "- \"Balanced accuracy\": simple average of per class conditional accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy conditional on class=0 (66.67% of examples) = 0.750\n",
      "Accuracy conditional on class=1 (33.33% of examples) = 0.500\n",
      "\n",
      "\n",
      "Computed Accuracy=0.667 ( 66.67% * 0.750 + 33.33% * 0.500 )\n",
      "Computed Balanced Accuracy=0.625 ( average( 0.75, 0.5) )\n"
     ]
    }
   ],
   "source": [
    "# Enumerate the classes\n",
    "classes = [0,1]\n",
    "\n",
    "accs, weights = [], []\n",
    "\n",
    "# Compute per class accuracy and fraction\n",
    "for c in classes:\n",
    "    # Filter examples and predictions, conditional on class == c\n",
    "    cond = y_true == c\n",
    "    y_true_cond, y_pred_cond = y_true[cond],  y_pred[cond]\n",
    "    \n",
    "    # Compute fraction of total examples in class c\n",
    "    fraction = y_true_cond.shape[0]/y_true.shape[0]\n",
    "    \n",
    "    # Compute accuracy on this single class\n",
    "    acc_cond = accuracy_score(y_true_cond, y_pred_cond)\n",
    "    \n",
    "    print(\"Accuracy conditional on class={c:d} ({p:.2%} of examples) = {a:3.3f}\".format(c=c, \n",
    "                                                                                        p=fraction,\n",
    "                                                                                        a=acc_cond\n",
    "                                                                                     )\n",
    "         )\n",
    "    \n",
    "    accs.append(acc_cond)\n",
    "    weights.append(fraction)\n",
    "\n",
    "# Manual computation of accuracy, to show the math\n",
    "acc = np.dot( np.array(accs), np.array(weights) )\n",
    "acc_bal = np.average( np.array(accs) )\n",
    "\n",
    "eqn_elts = [ \"{p:.2%} * {a:3.3f}\".format(p=fraction, a=acc_cond) for fraction, acc_cond in zip(weights, accs) ]\n",
    "eqn_bal = \" + \".join( eqn_elts )\n",
    "\n",
    "eqn = \"average( {elts:s})\".format(elts=\", \".join([ str(a) for a in accs ]))\n",
    "print(\"\\n\")\n",
    "print(\"Computed Accuracy={a:3.3f} ( {e:s} )\".format(a=acc, e=eqn_bal))\n",
    "print(\"Computed Balanced Accuracy={a:3.3f} ( {e:s} )\".format(a=acc_bal, e=eqn) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
