{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN vanishing/exploding gradient problem\n",
    "\n",
    "## Training Deep Networks is hard: Review\n",
    "As we learned in the module on Vanishing and Exploding Gradients\n",
    "- Training a very deep (many layer) network is difficult\n",
    "- Because as the gradient flows backwards (from Loss layer to Input layer)\n",
    "- The Loss Gradients successively either diminish or expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's quickly review the issue of vanishing and exploding gradients.\n",
    "\n",
    "Here is the picture of gradient flow during Back propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<center>Backward pass: Loss to Weights</center>\n",
    "<br>\n",
    "<img src=\"images/NN_Layers_plus_Loss_backward.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss Gradient of layer $\\ll$\n",
    "\n",
    "$$\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial \\y_\\llp}$$ \n",
    "\n",
    "flows backwards from Loss Layer $(L+1)$ inductively as:\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} & = & \\frac{\\partial \\loss}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, from the Loss Gradient and a local gradient $\\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}$ at layer $\\ll$\n",
    "- We can compute the derivative of the loss with respect to the layer's weights\n",
    "- Which is used in the update equation for Gradient Descent\n",
    "- To modify the estimate of the layer's weights\n",
    "- In the direction of decreasing Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<center>Forward and Backward pass: Detail</center>\n",
    "<br>\n",
    "<img src=\"images/Backward_pass_detail.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue arises in the second term $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$ of the inductive update of the Loss Gradient\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} \n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since\n",
    "$$\n",
    "\\y_\\llp = a_\\llp \\left( f_\\llp( \\y_{(\\ll-1)}, \\W_{\\llp}) \\right) \n",
    "$$\n",
    "\n",
    "The derivative\n",
    "$$\n",
    "\\begin{array}[lllll] \\\\\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "                                      & = a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "where\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "a'_\\llp & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp) )}{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)}  & \\text{derivative of } a_\\llp(\\ldots) \\text{ wrt } f_\\llp(\\ldots)\\\\\n",
    "f'_\\llp & = & \\frac{\\partial f_\\llp(\\y_{(\\ll-1)}, W_\\llp)}{\\partial \\y_{(\\ll-1)}} & \\text{derivative of } f_\\llp(\\ldots) \\text{ wrt } \\y_{(\\ll-1)}\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Substituting the value of the loss gradient into the backward update rule:\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\loss'_{(\\ll-1)} & = &  \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = &  \\loss'_\\llp a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We see that the backwards step from Loss Gradient of layer $\\ll$ to Loss Gradient of layer $(\\ll-1)$ introduces\n",
    "$a'_\\llp$ as a multiplicative term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But as we continue backwards (expanding $\\loss'_\\llp$ on the right hand side)\n",
    "we accumulate this multiplicative term\n",
    "\n",
    "Starting from layer $(L+1)$ and proceeding backwards to layer $\\ll$, the Loss Gradient term looks like\n",
    "$$\\loss'_\\llp  =   \\loss'_{(L+1)} \\prod_{l'=\\ll+1}^L  a'_{(l')} f'_{(l')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Specifically: it is the $a'_\\llp$ term that is problematic\n",
    "- If the activation functions $a_\\llp$ is such that $a'_\\llp < 1$:\n",
    "    - The backwards pass attenuates the Loss Gradient\n",
    "    - Eventually making it go to $0$ (disappear)\n",
    "- If the activation function $a_\\llp$ is such that $a'_\\llp > 1$:\n",
    " - The backwards pass amplifies the Loss Gradient\n",
    "    - Eventually making it go to $\\infty$ (explode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that \n",
    "- For $a_\\llp = \\sigma$ (the sigmoid function)\n",
    "- $\\max{z} a'_\\llp(z) = 0.25$  \n",
    "\n",
    "so using the sigmoid as the default activation\n",
    "- Made training of deep networks very difficult\n",
    "- Which stifled progress in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An unrolled RNN is a Deep Network\n",
    "\n",
    "If we unroll an RNN that has an input sequence of length $T$\n",
    "$$\n",
    "\\x_{(1)}, \\ldots, \\x_{(T)}\n",
    "$$\n",
    "\n",
    "we wind up with a network of $T$ layers (plus the Loss layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss Gradient Flow</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss_gradient.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As the input sequence length $T$ gets large\n",
    "- It should be no surprise that training an RNN\n",
    "- Is exposed to the problem of vanishing and exploding gradients\n",
    "- Because of the derivative of the activation function (written as $\\phi$ rather than $a_\\llp$ in the RNN literature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But it turns out that there is a *second* source of vanishing/exploding gradients for RNN's:\n",
    "- The weight matrix $\\W$ is shared at every step of the unrolled network\n",
    "\n",
    "Let's see how this can lead to vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing/Exploding gradients\n",
    "\n",
    "Let's recall the RNN update equations:\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \\\\\n",
    "\\y_\\tp & = &  \\W_{hy} \\h_\\tp  + \\b_y \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For simplicity of presentation: we will assume activation function $\\phi$ is the identity function in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Returning to the equation that derives the derivative of the Loss with respect to weights $\\W$:\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Let's focus on the term\n",
    "$$\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W}$$\n",
    "\n",
    "(replacing $\\ll$ as the index of the layer with $t$, the time step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will focus on the part of $\\W$ that is $\\W_{hh}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_\\tp}{\\partial \\W_{hh}} = \\frac{\\partial \\y_\\tp}{\\partial \\h_\\tp} \\frac{{\\partial \\h_\\tp}}{\\partial \\W_{hh}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This term comes about due to the RNN update equation\n",
    "$$\\y_\\tp  =  \\W_{hy} \\h_\\tp  + \\b_y$$\n",
    "- And $\\h_\\tp$ is a function of $\\W_{hh}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W} = \n",
    "\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W_{hy}}\n",
    "+\n",
    "\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\h_{(\\tt)} } \\frac{\\partial \\h_{(\\tt)}}{\\partial \\W_{hh}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's expand the term\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\W_{hh}}\n",
    "$$\n",
    "\n",
    "Since \n",
    "$$\n",
    "\\h_\\tp =  \\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(\\tt-1)}  + \\b_h\n",
    "$$\n",
    "\n",
    "$\\h_\\tp$ depends on $\\h_{(\\tt-1)}$, which by recursion depends on $\\h_{(\\tt-2)}$ which $\\ldots$ depends on $\\h_{(0)}$.\n",
    "- and all $\\h_\\tp$ share the *same* $\\W_{hh}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that $\\h_\\tp$ depends on $\\W_{hh}$ through *each* $\\h_{(\\tt-k)}$ for $k=1, \\ldots, \\tt$.\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\W_{hh}} = \n",
    "\\sum_{k=1}^{\\tt} { \n",
    " \\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt -k)}}   \\frac{\\partial \\h_{(\\tt -k)}}{\\partial \\W_{hh}}   \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problematic term for us is\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt-k)}}\n",
    "$$\n",
    "\n",
    "It can be computed by $k$ applications of the Chain Rule as\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt-k)}} &  = & \\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt -1)}}   \\frac{\\partial \\h_{(\\tt -1)}}{\\partial \\h_{(\\tt -2)}} \\ldots \\frac{\\partial \\h_{(\\tt -k + 1)}}{\\partial \\h_{(\\tt -k)}}\\\\\n",
    "& = & \\prod_{u=0}^{k-1} { \\frac{\\partial \\h_{(\\tt -u)}}{\\partial \\h_{(\\tt - u -1) }}} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each term\n",
    "$$\n",
    "\\frac{\\partial \\h_{(\\tt -u)}}{\\partial \\h_{(\\tt - u-1)}}\n",
    "$$\n",
    "results in a term $\\W_{hh}$ because\n",
    "\n",
    "$$\n",
    "\\h_\\tp =  \\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(\\tt-1)}  + \\b_h\n",
    "$$\n",
    "\n",
    "So the **repeated product is equal to the matrix $\\W_{hh}$ raised to the power $k$**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For simplicity, suppose $\\W_{hh}$ were a scalar (in general: use eigenvalues of matrices and matrix algebra)\n",
    "\n",
    "Raising $\\W_{hh}$ to the power of $k$\n",
    "- Approaches $0$ as $k$ increases, when $\\W_{hh} < 1$\n",
    "- Approaches $\\infty$ as $k$ increases, when $\\W_{hh} > 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words:\n",
    "- As the distance $k$ between time steps increases\n",
    "- The Loss Gradient tends to either vanish or explode\n",
    "- Inhibiting weight updates and learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If updates *do* occur, they will either be\n",
    "- Erratic (large loss gradients)\n",
    "- Slow (small loss gradients)\n",
    "\n",
    "Remember that this cause of vanishing/exploding gradients *is particular to* recurrent layers\n",
    "- Because of the sharing of weights between time steps\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "How is raising a matrix to a power related to eigenvalues ?\n",
    "\n",
    "Consider matrix $M$.  It's eigen decomposition is\n",
    "$$\n",
    "M = \\W \\Lambda \\W^{-1}\n",
    "$$\n",
    "\n",
    "where $\\Lambda$ is the *diagonal* matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array} \\\\\n",
    "M^p & = & M M^{p-1} \\\\\n",
    "& = &  (\\W \\Lambda \\W^{-1}) &  M^{p-1}  & \\text{since } M = (\\W \\Lambda \\W^{-1})\\\\\n",
    "& = &  (\\W \\Lambda \\W^{-1}) & M M^{p-2} & \\text{since }  M^{p-1} = M M^{p-2} \\\\\n",
    "& = &  (\\W \\Lambda \\W^{-1}) & (\\W \\Lambda \\W^{-1}) M^{p-2}  & \\text{since } M = (\\W \\Lambda \\W^{-1})\\\\\\\\\n",
    "& = &  (\\W \\Lambda \\W^{-1}  \\W \\Lambda \\W^{-1}) M^{p-2} & \\text{associativity of multiplication} \\\\\n",
    "& = &  (\\W \\Lambda^2 \\W^{-1}) M^{p-2}  & \\text{since } \\W \\W^{-1} = I, \\Lambda \\Lambda = \\Lambda^2 \\\\\n",
    "\\vdots\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So you can see that raising $M$ to the power $p$ results in diagonal matrix $\\Lambda$ being raised to $p$\n",
    "- \\Which is just a diagonal matrix whose elements are the *scalar* diagonal elements of $\\Lambda$ raised to $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Controlling exploding gradients by clipping\n",
    "In theory, we can control the explosion by clipping the gradient $\\frac{\\partial \\loss}{\\partial W_i}$.\n",
    "\n",
    "We are still left with the vanishing gradient problem.\n",
    "\n",
    "This means that \"vanilla\" RNN's\n",
    "have difficulty learning\n",
    "long-term dependencies (i.e., too many steps backward).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Recurrent layers are especially exposed to the problem of Vanishing and Exploding gradients\n",
    "- As potentially very deep networks in the unrolled form\n",
    "- Due to sharing weights $\\W$ across time steps\n",
    "\n",
    "We will introduce some architectural innovations in Recurrent layers to ameliorate this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.582px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
