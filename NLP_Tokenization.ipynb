{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tokenizers\n",
    "\n",
    "The first step in the NLP pipeline is converting sequences of text into sequences of tokens.\n",
    "\n",
    "This process is called *Tokenization*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Fortunately, many toolkits provide implementations of Tokenizers so that you don't have to do this yourself.\n",
    "\n",
    "Our concern is to discuss the concepts and limitations.\n",
    "\n",
    "We refer the interested read to some good *practical* resources on Tokenization\n",
    "- [Hugging Face course section on Tokenization](https://huggingface.co/course/chapter2/4?fw=pt)\n",
    "- [TensorFlow `tensorflow_text` API](https://www.tensorflow.org/text)\n",
    "- [Keras Project NLP API](https://keras.io/api/keras_nlp/tokenizers/)\n",
    "    - Note: this is the Keras project **not** the implementation of Keras in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word based tokenization\n",
    "\n",
    "There are several varieties of Tokenization.\n",
    "\n",
    "The first is Word Based\n",
    "- convert words into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It would be naive to imagine that each word is a token\n",
    "- The size of the vocabulary $\\Vocab$ can easily be tens of thousands.\n",
    "- Not only are there many words, there are *variations* of each word\n",
    "    - verbs: past, present and future tense\n",
    "    - nouns: \n",
    "        - singular, plural\n",
    "        - gender agreement with subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Classical\" NLP has developed techniques to eliminate word variation by conversion to \"canonical\" form\n",
    "- stemming: convert word to its root form (root may not be in dictionary)\n",
    "    - drive, driver, driving $\\mapsto$ driv\n",
    "- lemmatization: convert word to a root that is in dictionary\n",
    "    - him: $\\mapsto$ he\n",
    "    - took $\\mapsto$ take\n",
    "    - earlier $\\mapsto$ early\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So one way to reduce the vocabulary size is by using these techniques.\n",
    "\n",
    "There is still a place for this techniques in Deep Learning.\n",
    "\n",
    "- [spaCy](https://spacy.io/) is a very popular toolkit for dealing with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is also possible that\n",
    "Embeddings will create a small number of \"dimension\" (e.g., variations) that are common to many words\n",
    "- the \"plural\" dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues with Word based tokenization\n",
    " \n",
    "Most word based tokenizers assumed that words are delimited by the space and a fixed number of\n",
    "punctuation characters.\n",
    "\n",
    "\n",
    "A big unresolved issue with word based tokenization:\n",
    "- No matter how large the vocabulary $\\Vocab$\n",
    "- There will still be words that are Out of Vocabulary (OOV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One way to address this is with Character based tokenization\n",
    "- each character is a token\n",
    "\n",
    "But this may inhibit learning\n",
    "- Characters are too low level\n",
    "- Compared to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sub-word Tokenization\n",
    "\n",
    "An elegant solution to the OOV issue is *sub-word tokenization*\n",
    "- break an OOV token into pieces, each of which is in-vocabulary\n",
    "    - here's $\\mapsto$ here, \\', s\n",
    "    - tokenizer $\\mapsto$ token, ##izer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some of the objectives are\n",
    "- to **not** split frequently occurring words into pieces\n",
    "    - So that the model consuming the tokens has access to meaningful single word pieces\n",
    "- to recognize pieces that are common prefix and suffix forms\n",
    "    - So that the model consuming the tokens can derive the commonality indicated by the suffix\n",
    "        - e.g. \"izer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sub-word tokenization  has become fairly common in modern NLP systems\n",
    "- Balances brevity of character encoding\n",
    "- With expressiveness of word encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sub-word tokenization:  Byte Pair Encoding (BPE)\n",
    "\n",
    "There are several methods to achieve sub-word Tokenization.\n",
    "\n",
    "A simple version is derived from [*Byte Pair Encoding* (BPE)](https://arxiv.org/pdf/1508.07909.pdf)\n",
    "- BPE is a way to replace common sequences of bytes with a short symbol\n",
    "- Word segmentation: the analog of BPE, where we replace common sequences of tokens with a short symbol\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Universal model: adapting task-specific inputs</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"http://twimgs.com/ddj/cuj/images/cuj9402gage/fig1.gif\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: http://twimgs.com/ddj/cuj/images/cuj9402gage/fig1.gif\"</center></td>\n",
    "    </tr>   \n",
    "</table>\n",
    "\n",
    "- Pass 1: AB $\\mapsto$ H\n",
    "    - byte sequence \"A\", \"B\" occurs 3 times in input\n",
    "    - replaced with symbol \"H\" (recorded dictionary entry \"H\"_\n",
    "- Pass 2: HC $\\mapsto$ G\n",
    "    - symbol sequence \"H\", \"C\" (occurring twice) replaced with symbol \"G\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## WordPiece sub-word tokenization\n",
    "\n",
    "[WordPiece](https://arxiv.org/pdf/1609.08144.pdf) is a variant of BPE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence Piece sub-word tokenization\n",
    "\n",
    "BPE provides a unique encoding (*segmentation*) of a sentence of text.\n",
    "\n",
    "But there are multiple ways of dividing words into segments.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/Subword_segmentation_not_unique.png\">\n",
    "    <br>\n",
    "    <center>Attribution: Table 1 in https://arxiv.org/pdf/1804.10959.pdf</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The paper [Subword Regularization: Improving Neural Network Translation Models\n",
    "with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf) proposed\n",
    "- using multiple segmentations\n",
    "- associating a probability with each\n",
    "\n",
    "The advantages include\n",
    "- accommodation of multiple languages\n",
    "- increased robustness of training\n",
    "\n",
    "[SentencePiece](https://arxiv.org/pdf/1808.06226.pdf) is an Open Source implementation of the\n",
    "subword regularization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
