{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "**Week 2 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Regression task: Linear Regression\n",
    "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    \n",
    "**The Recipe for Machine Learning, illustrated with the Linear Regression model**  \n",
    "- [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2\n",
    "**Plan**\n",
    "\n",
    "We will continue learning the Recipe for Machine Learning, illustrated by introducing the Linear Regression model for solving Regression tasks.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Recipe for Machine Learning/Linear Regression** (continued)\n",
    "\n",
    "- We continue with Step 2 of the Recipe: [Exploratory Data Analysis](Recipe_Overview.ipynb#Exploratory-Data-Analysis)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "    \n",
    "    \n",
    "- [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    " \n",
    "**Deeper dives**\n",
    "- Iterative improvement\n",
    "    - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "        - Regularization\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n",
    "\n",
    "**Week 3 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Classification task: Logistic Regression\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**Regression wrap-up**\n",
    "\n",
    "A few missing items/clarifications from last week\n",
    "- Use linear regression for hedging (clarification).  I gave the linear regression example\n",
    "$$\n",
    "r_\\text{AAPL} \\approx \\Theta_0 + \\Theta_1 * r_\\text{SPX}\n",
    "$$\n",
    "    - The return of AAPL is $\\Theta_1$ times as much as the return of the index SPX\n",
    "    - So if I hold \\\\$1 of AAPL long, I would short $\\Theta_1$ of SPX in order to neutralize market exposure\n",
    "    - Often assume $\\Theta_0 = 0$\n",
    "    - Remember: the fit involves an \"error\"\n",
    "    $$\n",
    "r_\\text{AAPL} = \\Theta_0 + \\Theta_1 * r_\\text{SPX} + \\epsilon\n",
    "$$\n",
    "    - The hedged portfolio returns $\\epsilon$\n",
    "    - If, through my modeling, I conclude that $\\epsilon > 0$, I can profit regardless of the direction of SPX\n",
    "\n",
    "- Where do regression errors come from ?\n",
    "    - Measurement error, noise\n",
    "    - Missing feature\n",
    "    \n",
    "- [Regularization: a way to combat overfitting](Bias_and_Variance.ipynb#Regularization:-reducing-overfitting)\n",
    "\n",
    "**Plan**\n",
    "- We continue our Introduction of a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "<img src='images/class_overview_prob_lines.jpg'>\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview (continued)](Classification_Overview.ipynb#Logistic-Regression)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Classification, continued**\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "**Plan**\n",
    "- Classsification and Categorical variables wrapup\n",
    "    - Baseline models: a baseline model for classification\n",
    "    - OHE and Linear Regression: The Dummy Variable Trap\n",
    "- Error Analysis\n",
    "\n",
    "**Classsification and Categorical variables wrapup**\n",
    "- [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "\n",
    "\n",
    "**Wrap-up from last week: Multinomial classification, illustrated**\n",
    "\n",
    "Last week, we indicated how we could use a collection of binary classifiers to create a classifier for more than $2$ classes.\n",
    "\n",
    "[Here](Multinomial_Classification.ipynb#Multinomial-classification-example:-MNIST-digit-classifier)\n",
    "is an example to illustrate the keys concepts, as well as better understand the $\\Theta$ as patterns being matched in the input examples\n",
    "\n",
    "**This week**\n",
    "\n",
    "Good news\n",
    "- You now know two main tasks in Supervised Learning\n",
    "    - Regression, Classification\n",
    "- You now know how to use virtually every model in `sklearn`\n",
    "    - Consistent API\n",
    "        - `fit`, `transform`, `predict`\n",
    "- You survived the \"sprint\" to get you up and running with ML\n",
    "- You know the *mechanical process* to implement transformations: Pipelines\n",
    "    \n",
    "**Plan**\n",
    "- Error Analysis\n",
    "    - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "    - How Training Loss can be improved\n",
    "- Transformations, continued\n",
    "    - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "- Loss functions\n",
    "    - We look at the mathematical logic behind loss functions\n",
    "\n",
    "**Error Analysis**\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "**Transformations, continued**\n",
    "- [Transformations Overview](Transformations_Overview.ipynb)\n",
    "\n",
    "**Imbalanced data**\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "\n",
    "**Loss functions: mathematical basis**\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "- Naive Bayes: a simple but effective model\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "**Week 6 (early start)**\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6   \n",
    "\n",
    "**Plan**\n",
    "- We continue with Unsupervised Learning: Principal Components (started last week)\n",
    "- Support Vector Classifiers: a classifier with an interesting twist \n",
    "- Gradient Descent: how to minimize a Loss function\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- **Done** [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb#PCA:-The-math)(continued)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "  \n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "**Gradient Descent** \n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7\n",
    "\n",
    "**Plan**     \n",
    "- Interpretation: understanding models\n",
    "\n",
    "**Interpretation**\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "  \n",
    "**Deeper dives**\n",
    "\n",
    "## Deep Learning: Headstart\n",
    "\n",
    "### Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
