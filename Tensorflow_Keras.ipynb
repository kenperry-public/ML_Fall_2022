{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Keras\n",
    "\n",
    "In this module we will introduce [Keras](https://keras.io/), a high level API for Neural Networks.\n",
    "\n",
    "\n",
    "To be specific\n",
    "- we will mostly restrict ourselves to the Keras Sequential model\n",
    "- this will greatly simplify your learning and coding\n",
    "- it will restrict the type of Deep Learning programs that you can write\n",
    "    - but not a meaningful restriction for the simple programs that you will write in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After we introduce the high level Keras API\n",
    "- we will review the history of Deep Learning programming to see how we got here\n",
    "- this will give you greater insight into what Keras does \"under the covers\"\n",
    "    - appreciate history\n",
    "    - aid your diagnostics\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**: \n",
    "\n",
    "The code snippets in this notebook are *fragments* of a larger [notebook](DNN_TensorFlow_example.ipynb)\n",
    "- are illustrative: will not actually execute in this notebook but will in the complete notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Confusion warning:\n",
    "- There are two similar *but different* packages that implement Keras\n",
    "    - one built into TensorFlow (the one we will use)\n",
    "    - a separate project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <br>\n",
    "        <b>YES</b><br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <font face=\"Courier\" color=\"black\" weight=\"bold\">\n",
    "            import tensorflow as tf<br>\n",
    "            tf.keras.layers.Dense(...)\n",
    "            </font>\n",
    "        </li>\n",
    "        <li>\n",
    "            <font face=\"Courier\" color=\"black\" weight=\"bold\">\n",
    "            from tensorflow import keras<br>\n",
    "            keras.layers.Dense(...)\n",
    "            </font>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <br>\n",
    "        <b>NO</b><br>\n",
    "    <ul>\n",
    "    <li> <font face=\"Courier\" color=\"black\" weight=\"bold\">\n",
    "        import keras<br>\n",
    "        keras.layers.Dense( ... )\n",
    "        </font>\n",
    "        </li>\n",
    "    </ul>\n",
    "        </div>\n",
    "        \n",
    "If you want to know the details, visit this [notebook](Tensorflow_Keras_Archaeology.ipynb#tensorflow.keras-vs-keras-(Confusion-alert))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Keras Sequential Model\n",
    "\n",
    "**Reference**: [Getting started with the Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "\n",
    "Keras has two programming models\n",
    "- Sequential\n",
    "- Functional\n",
    "\n",
    "We will start with the Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Sequential model allows you to build Neural Networks (NN) that are composed of a *sequence* of layers\n",
    "- just like our cartoon\n",
    "- a very prevalent paradigm\n",
    "\n",
    "This will likely be sufficient in your initial studies\n",
    "- but it restricts the architecture of the Neural Networks that  you can build\n",
    "- use the Functional API for full generality\n",
    "    - but it might appear more complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's jump into some code.\n",
    "\n",
    "Some old friends, in new clothing:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Regression\n",
    "reg_model = Sequential([ layers.Dense(1, activation=None, input_shape=X[0].shape) ] )\n",
    "\n",
    "reg_model.compile(optimizer='rmsprop', loss='mse')\n",
    "reg_model.fit(x, y, nb_epoch=10, validation_data=(x_val, y_val ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A model uses the `Sequential` architecture\n",
    "- A sequence (implemented as an array) of layers\n",
    "    - Single element array\n",
    "    - Consisting of a `Dense` (Fully connected) layer\n",
    "        - with $1$ output\n",
    "        - No activation\n",
    "        - Implements Regression\n",
    "- Loss is `mse`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classification\n",
    "class_model = Sequential([ layers.Dense(1, activation=\"sigmoid\", input_shape=X[0].shape) ] )\n",
    "\n",
    "class_model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "class_model.fit(x, y, nb_epoch=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A model uses the `Sequential` architecture\n",
    "- A sequence (implemented as an array) of layers\n",
    "    - Single element array\n",
    "    - Consisting of a `Dense` (Fully connected) layer\n",
    "        - with $1$ output: binary classification\n",
    "        - sigmoid activation\n",
    "        - Implements Classification\n",
    "- Loss is `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Both examples are a single layer\n",
    "            <ul>\n",
    "                <li>Dense, with 1 unit (\"neuron\")</li>\n",
    "            </ul>\n",
    "        <li>Regression example\n",
    "             <ul>\n",
    "                <li>No activation</li>\n",
    "                <li>MSE loss</li>\n",
    "                 </ul>\n",
    "        <li>Binary classification example\n",
    "         <ul>\n",
    "            <li>Sigmoid activation</li>\n",
    "            <li>Binary cross entropy loss</li>\n",
    "             </ul>\n",
    "    </ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully you get the idea.\n",
    "\n",
    "Let's explore a slightly more complicated model.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "mnist_model = Sequential([ \n",
    "    layers.Dense(n_hidden_1, activation=tf.nn.relu,    name=\"hidden1\", input_shape=X[0].shape),\n",
    "    layers.Dense(n_hidden_2, activation=tf.nn.relu,    name=\"hidden2\")\n",
    "    layers.Dense(output_size,activation=tf.nn.softmax, name=\"outputs\")\n",
    "                         ]\n",
    "                        )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A model uses the `Sequential` architecture\n",
    "- A sequence (implemented as an array) of layers\n",
    "    - 3 layers (3 element array)\n",
    "    - 2 `Dense` layers \n",
    "        - with varying number of outputs: `n_hidden_1`,`n_hidden_2`\n",
    "        - `relu` activation\n",
    "    - A `Dense` layer implementing Multinomial Classification\n",
    "        - number of outputs equal to number of classes\n",
    "        - `softmax` activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The first two layers \"transform\" the input\n",
    "- The \"head\" layer implements Multinomial Classification\n",
    "\n",
    "To use the model, you first need to \"compile\" it"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "metrics = [ \"acc\" ]\n",
    "mnist_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Compiling\" is quite significant as we will demonstrate later\n",
    "- For now: *it is where you define the Loss function*\n",
    "    \n",
    "Next, just as in `sklearn`: you \"fit\" the model to the training data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "history = mnist_model.fit(X_train, y_train, \n",
    "                          epochs=n_epochs, \n",
    "                          batch_size=batch_size, \n",
    "                          validation_data=(X_valid, y_valid), \n",
    "                          shuffle=True\n",
    "                         )\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once the model is fit, you can predict, just like `sklearn`.\n",
    "\n",
    "Here we evaluate the model on the Test dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "test_loss, test_accuracy = mnist_model.evaluate(X_test, y_test)\n",
    "print(\"Test dataset: loss={tl:5.4f}, accuracy={ta:5.4f}\".format(tl=test_loss, ta=test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is quite simple\n",
    "- Keras Sequential implements an `sklearn`-like API\n",
    "    - define a model\n",
    "    - fit the model\n",
    "    - predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have glossed over a lot of details\n",
    "- What does each layer do ?\n",
    "- Why do we need to \"compile\" ?\n",
    "    - and why does it need an optimizer ?\n",
    "\n",
    "We will explain some of these details as part of our explanation of the code in our first example notebook (last section of this module)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential model summary\n",
    "- A Sequential model consists of an array of layers\n",
    "- The array may be constructed in several ways\n",
    "    - By passing it to the `Sequential` constructor\n",
    "    - By appending one layer at a time to an existing model\n",
    "    \n",
    "    `model.add( Dense(10, activation=\"relu\") )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first layer of a Sequential model is special.\n",
    "\n",
    "**Best practice** is to provide an indication of the size of an example as part of the first layer's definition\n",
    "- Usually the `input_shape` argument (but sometimes different)\n",
    "\n",
    "    `Sequential( [ Dense(10, activation=\"relu\", input_shape=X[0].shape ] )`\n",
    "    \n",
    "    - where `X` is the matrix of training examples\n",
    "        - so `X[0].shape` is the shape of the first (and hopefully each) example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is **not** necessary (but still a good idea) to specify the input shape.  If you do not\n",
    "- You can't examine the model, e.g.\n",
    "\n",
    "    `model.summary()`\n",
    "- until **after** you first call it\n",
    "    - the shape of the input will be inferred from the first example with which the model is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Technical note**\n",
    "\n",
    "The number of weights of the first layer depends on the shape of an input example.\n",
    "- If you provide the shape: it is known immediately\n",
    "- If you do not provide the shape, it will not be known until the model is first run\n",
    "    - inferred from first example\n",
    "\n",
    "The number of weights of layers subsequent to the first can be inferred from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Keras Functional Model\n",
    "\n",
    "- More verbose than `Sequential`\n",
    "- Also more flexible\n",
    "    - you can define more complex computation graphs (multiple inputs/outputs, shared layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(32, activation='relu')(inputs)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and  Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Highlights:\n",
    "- Manually invoke a single layer at a time\n",
    "    - Passing as input the output of the prior layer.\n",
    "\n",
    "- You must define an `Input` layer (placeholder for the input/define its shape)\n",
    "    - `Sequential` uses the `input_shape=` parameter to the first layer\n",
    "- You \"wrap\" the graph into a \"model\" by a `Model` statement\n",
    "    - looks like a function definition\n",
    "        - names the input and output formal parameters\n",
    "    - a `Model` acts just like a layer (but with internals that you create)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)  # starts training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a beginner, you will probably exclusively use the Sequential model.\n",
    "\n",
    "Keep the Functional API in the back of your mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code !\n",
    "\n",
    "Lets see a working notebook.\n",
    "\n",
    "Two options\n",
    "- Run on your local machine: [DNN Tensorflow example Notebook local](DNN_TensorFlow_example.ipynb) (local)\n",
    "    - Tensorflow version 2+ only !\n",
    "- Run on Google Colab: [DNN Tensorflow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2022/blob/master/DNN_TensorFlow_example.ipynb) (**Colab**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
