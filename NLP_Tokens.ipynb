{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues with text: Sparse encoding of tokens\n",
    "\n",
    "A token is an element of the finite set $\\Vocab$\n",
    "- It is thus a *categorical* variable\n",
    "- Which is naturally represented by a vector of length $|| \\Vocab ||$\n",
    "    - One Hot Encoded (OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, in theory, we already know how to perform NLP\n",
    "- Encode text as a sequence of OHE vectors\n",
    "- Apply techniques (e.g., RNN) specialized to sequences\n",
    "\n",
    "This approach may be both\n",
    "- Impractical\n",
    "- Sub-optimal: not taking advantage of properties inherent in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us suppose that we had a function that maps a token to a OHE vector\n",
    "$$\\text{rep}: \\text{token} \\mapsto \\mathbb{R}^{n_\\Vocab}\n",
    "$$\n",
    "\n",
    "Thus\n",
    "$$\\text{rep}(\\w_\\tp)$$\n",
    "is the OHE vector for $\\w_\\tp \\in \\Vocab$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first issue we encounter:\n",
    "- $\\Vocab$ is big ! A decent vocabulary is easily thousands of token\n",
    "- So the OHE is a *long* vector ($|| \\Vocab ||$)\n",
    "\n",
    "Thus, OHE may not be *practical*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is also potentially failing to take advantage of relationships between tokens\n",
    "- There is clearly a relationship between tokens\n",
    "$$\n",
    "\\text{dog}, \\text{dogs}\n",
    "$$\n",
    "but *no* relationship between their OHE vectors\n",
    "$$\n",
    "\\text{rep}(\\text{dog}), \\text{rep}(\\text{dogs})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OHE vectors are *sparse*\n",
    "- All zero\n",
    "- Except for a single element\n",
    "\n",
    "The sparsity both wastes space and is the cause for not capturing potential relationships between words\n",
    "\n",
    "We will subsequently demonstrate a *dense* encoding of tokens that solves both issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues with text: variable length sequences\n",
    "\n",
    "Another issue with text: the sequence length is variable, not constant.\n",
    "\n",
    "The reason this may be an issue\n",
    "- Classical Machine Learning models (e.g. Logistic Regression) can only deal with fixed length inputs\n",
    "- The final layer in a Neural Network is usually an implementation of a Classical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately, we can easily identify two solutions to this issue.\n",
    "\n",
    "Both involve reducing a variable length sequence to a fixed length encoding.\n",
    "\n",
    "Once tokens have been encoded as a sequence of numeric vectors the solutions are\n",
    "- Replace the variable number of tokens by a summary statistic (sum/average) over the tokens\n",
    "- Use the final state of an RNN as an encoding of the sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a notational convenience\n",
    "we will extend $\\text{rep}$ to sequences $\\w$:\n",
    "$$\\text{rep}(\\w) = \\left[ \\text{rep}(\\w_\\tp) | 1 \\le \\tt \\le ||\\w||  \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional methods for summarizing a variable length sequence\n",
    "\n",
    "The simplest way to derive a fixed length encoding of a sequence is by a summarization operation.\n",
    "\n",
    "This is the approach that was pursued in \"traditional\" (pre-Neural Network) NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall what a Global Pooling layer does\n",
    "- Each row is a feature over multiple spatial/temporal locations (tokens in the sequence)\n",
    "- Is transformed to a single value\n",
    "- Preserving the feature/channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>Global Pooling<br>3 features over spatial locations<br>to 3 features over one location</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/NLP_Global_Pooling.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unfortunately, the summarization (sum/average) of a sequence\n",
    "- Will lose the ordering relationship among the tokens\n",
    "\n",
    "Consider the summarization across single tokens of the two sequences\n",
    ">\"Machine Learning is *easy not hard*\"\n",
    "\n",
    ">\"Machine Learning is *hard not easy*\"\n",
    "\n",
    "both have the same summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will give a brief introduction to traditional summarization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of Words (BOW): Pooling\n",
    "\n",
    "We define a *reduction* operation $\\text{CBOW}$\n",
    "- convert a sequence $\\w$ of $||\\w||$ elements\n",
    "- each element of length $|| \\text{rep}(\\w_\\tp) ||$\n",
    "- to a fixed length vector of length $|| \\text{rep}(\\w_\\tp) ||$\n",
    "\n",
    "This will necessarily lose token order: this method is called *Bag of Words (BOW)*\n",
    "\n",
    "There are many operators to achieve the reduction, which we will group under the name *pooling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sum/Average\n",
    "$$\n",
    "\\text{CBOW}(\\w) = \\sum_{\\tt=1}^{||\\w ||} {  \\text{rep}(\\w_\\tp) }\n",
    "$$\n",
    "\n",
    "Since $\\w_\\tp$ is a vector, the addition operation is element-wise.\n",
    "\n",
    "So the composite vector for the sequence is the sum of the vectors of each element in the sequence.\n",
    "\n",
    "We can easily turn the Sum into an average by dividing by $||\\w||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Count vectorization:  \n",
    "\n",
    "In the special case that \n",
    "$$\\text{rep}(\\w_\\tp) = \\text{OHE}(\\w_\\tp)\n",
    "$$\n",
    "\n",
    "$\\text{CBOW}(\\w)_j$ is equal to the number of occurrences in sequence $\\w$ of the $j^{th}$ word in $\\Vocab$.\n",
    "\n",
    "This is often called *Count Vectorization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### TF-IDF\n",
    "\n",
    "Count Vectorization is simple but ignores a basic fact or language\n",
    "- Word \"importance\" is often inversely correlated with frequency in $\\Vocab$\n",
    "\n",
    "In English: \n",
    "- The words \"a\", \"the\" and \"is\" are extremely high frequency (so high counts in most sequences $\\w$).\n",
    "- But are so common as to convey little meaning\n",
    "\n",
    "On the other hand, a rare word (or sequence of words) may be very distinctive (\"Machine Learning\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Term Frequency, Inverse Document Frequency (TF-IDF)* \n",
    "- Is based on the idea that\n",
    "a word that is *infrequent* in the wide corpus (collection of text)\n",
    "- But is frequent in a particular document (one text in the collection) in\n",
    "the corpus is very meaningful in the context of the document.\n",
    "\n",
    "So a document \n",
    "- In which \"Machine Learning\" occurred a disproportionately high (relative to the broad corpus)\n",
    "number of times \n",
    "- Is likely to indicate that the document is dealing with the subject of Machine Learning.\n",
    "\n",
    "**Note** A similar idea is behind many Web search algorithms (Google)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TF-IDF is similar to the Count Vectorizer, but with modified counts \n",
    "that are the product of \n",
    "- The frequency of a token within a single document\n",
    "- The inverse of the frequency of the token relative to all documents\n",
    "\n",
    "- $v$ is a token\n",
    "- $d$ is a document (collection of tokens) in set of documents $D$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\text{tf}(v,d) & = & \\text{frequency of word } v \\text{ in document } d & \\text{(Term Frequency)}\\\\\n",
    "\\text{df}(v) & = & \\text{number of documents that contain word } v \\\\\n",
    "\\text{idf}(v) & = & \\log( \\frac{ ||D|| } { \\text{df}(v) } ) + 1 & \\text{Inverse Document Frequency} \\\\\n",
    "\\\\\n",
    "\\text{tf-idf}(v,d) & = & \\text{tf}(v, d) * \\text{idf}(v) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, $\\text{tf-idf}(v,d)$ is high (token $v$ is *important* to document $d$):\n",
    "- For tokens $v$ that occur infrequently in the corpus (resulting in high inverse: $\\text{idf}(v)$)\n",
    "- But which occur frequently in a particular document $d$: $\\text{tf}(v, d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using an RNN to obtain a fixed length encoding of a variable length sequence\n",
    "\n",
    "As a refresher, here is our picture of the RNN API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>RNN loop</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop_NLP.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although we don't know exactly what $\\h_\\tp$ represents, recall that\n",
    "- It *is* a summary of the prefix of input $\\x$ ending at step $\\tt$\n",
    "\n",
    "Thus $\\h_{(T)}$ is a summary of sequence $[\\x_{(1)}, \\ldots, \\x_{(T)} ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It will be typical to see a Neural Network for an NLP task having an architecture\n",
    "- That uses an RNN to process input $\\x$\n",
    "- Followed by other Layers\n",
    "- Culminating in \"head\" layer $L$ implementing a Classical Layer (e.g., Logistic Regression)\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>RNN Many to one; followed by classifier</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Text data is characterized by\n",
    "- Tokens that are categorical variables\n",
    "- Variable length sequences of tokens\n",
    "\n",
    "We presented some \"obvious\" methods to deal with both issues, but they clearly have limitations.\n",
    "\n",
    "We will move beyond the limitations in a subsequent module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
