{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The \"Predict the Next\" task\n",
    "\n",
    "Many tasks involving sequences start off as many to one:\n",
    "- takes a short sequence of words (the \"seed\")\n",
    "- outputs a prediction for a probable next word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to train a model to \"predict the next\" element we construct a training set\n",
    "- The features are sequences\n",
    "- The target is the next element in the sequence\n",
    "\n",
    "\n",
    "Let \n",
    "$$[ \\;\\mathbf{s}_\\tp | 1 \\le \\tt \\le T \\; ]$$\n",
    "be the elements of sequence $\\mathbf{s}$.\n",
    "\n",
    "We will prepare $(T-1)$ training examples from this single sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\langle \\X, \\y \\rangle = $\n",
    "\n",
    "$\n",
    "\\begin{array} \\\\\n",
    "  i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "  \\hline \\\\\n",
    "  1 & \\mathbf{s}_{(1) }  & \\mathbf{s}_{(2)} \\\\\n",
    "  2 & \\mathbf{s}_{(1), (2) }  & \\mathbf{s}_{(3)} \\\\\n",
    "  \\vdots \\\\\n",
    "  i & \\mathbf{s}_{(1), \\ldots, (i) }  & \\mathbf{s}_{(i+1)} \\\\\n",
    "  \\vdots \\\\\n",
    "  (T-1) & \\mathbf{s}_{(1), \\ldots, (T-1) }  & \\mathbf{s}_{(T)} \\\\\n",
    "  \\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, given a sequence of words\n",
    "\n",
    "$\\mathbf{s} = $\n",
    "\"I am taking a class in Machine Learning\"\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{array}\\\\\n",
    "i & \\x^\\ip  & \\y^\\ip \\\\\n",
    "1 & [\\;  \\text{I} \\; ] & \\text{am} \\\\\n",
    "2 & [\\; \\text{I, am} \\; ] & \\text{taking} \\\\\n",
    "3 & [\\; \\text{I, am, taking} \\; ] & \\text{a} \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Being able to predict the next element may be key to understanding the \"logic\" underlying a sequence\n",
    "- You have to understand context and domain\n",
    "- You have to understand how earlier elements influence latter elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi Supervised Learning\n",
    "\n",
    "Where do the raw sequences $\\mathbf{s}$ come from ?\n",
    "\n",
    "They may be all around us !  This is especially true for sequences of words\n",
    "- Each article (news, Wikipedia, etc.) is a collection of sequences (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the raw sequences are not pairs of Feature/Target.\n",
    "- You have to transform it into that form\n",
    "\n",
    "Thus, we refer to this type of learning as *Semi-Superivised*\n",
    "- transforming unlabeled data into labeled examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The transformation of raw text examples into structured Feature/Target examples\n",
    "comprise the \"Prepare the Data\" step in our Recipe.\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/EdX/W10_L2_ML_process.png\" width=70%</td> <!Derived by EdX from images/ML_process.jpg>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is usually the case that Sequence data involves substantial Data Preparation.\n",
    "\n",
    "Suppose our task is to predict the next word in a sentence.\n",
    "\n",
    "We are given (or must obtain) a collection of sentences (e.g., one or more documents) as our raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But a sentence is not the format required for the training set of the \"Predict the next word\" task.\n",
    "\n",
    "Data preparation is usually a substantial prerequisite for solving tasks involving sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be precises, the \"Predict the next word\" task involves\n",
    "- Training a many to one RNN with examples created from a sequence.\n",
    "- The elements of a single example are the prefix of a sentence\n",
    "- The target of the example is the next word in the sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict the next: data shape\n",
    "\n",
    "We had warned earlier about the explosion of the number of dimensions of our data.\n",
    "Now is a good time to take stock\n",
    "- $\\X$, the training set, is a matrix with $m$ rows\n",
    "- Each row is an example $\\x^\\ip$\n",
    "- Each example is a sequence $[ \\; \\x^\\ip_\\tp \\, | \\, 1 \\le \\tt \\le || \\x^\\ip || \\; ]$\n",
    "- Each element $\\x^\\ip_\\tp$ of the sequence encodes a word\n",
    "- A word is encoded as a One Hot Encoded binary vector of length $|| V||$ where $V$ is the set of words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Target $\\y^\\ip$ is also a word (so is  vector of length $|| V||$).\n",
    "- Many to one: target is *not* a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predict the next: training\n",
    "\n",
    "Just like training any other type of layer, but more expensive\n",
    "- Each example involves multiple time steps: forward pass is time consuming\n",
    "- The derivatives (needed for Gradient Descent) are more complex; backward pass complex and time consuming\n",
    "\n",
    "Remember:\n",
    "- the target $\\y_\\tp$ for step $t$ should be $\\x_{(\\tt+1)}$ the next input\n",
    "$$\n",
    "\\y_\\tp = \\x_{(\\tt+1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Predict the next: Training</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a generative model (fun with RNN's)\n",
    "\n",
    "The \"Predict the next\" word task is interesting on its own.\n",
    "\n",
    "But a slight twist will make it extremely interesting\n",
    "\n",
    "- Suppose we append the prediction to the input sequence\n",
    "- And feed the extended sequence back into the model\n",
    "- Repeat !  We can extend the initial short \"seed\" sequence to arbitrary length.\n",
    "\n",
    "This is how the example of generating a story from a seed idea works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the latent state summarizes the entire prefix of a sequence\n",
    "- We don't have to feed the entire extended sequence in as input\n",
    "- We just need to feed the newly generated \"next\" element into the RNN whose latent state has already encoded the prefix\n",
    "\n",
    "Once the model has been trained, we can generate increasingly longer sequences at test time as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At test time, we feed a short \"seed\" sentence\n",
    "$$\n",
    "\\x_{(0)}, \\ldots, \\x_\\tp\n",
    "$$\n",
    "into the model and have it generate output.\n",
    "\n",
    "**But** \n",
    "we then feed the output back into the model as input !\n",
    "$$\n",
    "\\x_{(\\tt'+1)} = \\y_{(\\tt')} \\; \\text{for } \\tt' \\ge \\tt\n",
    "$$\n",
    "\n",
    "- as in the Decoder in our Language Translation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_one_to_many.png\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model would generate new text ad infinitum\n",
    "- The next word generated would be based on what the model has learned from training\n",
    "- To be the most probable word to follow the prefix\n",
    "\n",
    "The extended sequences would thus be highly probable sequences from the same domain as the training examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating strange things\n",
    "\n",
    "Generating stories from seeds was very popular a few years back.\n",
    "\n",
    "Let's look at some examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But first, a surprise:\n",
    "- Rather than solving a \"predict the next word\" task\n",
    "- All of the following examples were generated by a \"predict the next **character**\" task !\n",
    "\n",
    "The choice was motivated by practical considerations\n",
    "- Both characters and words are categorical values and need to be One Hot Encoded\n",
    "- The number of possible text characters is much smaller than the number of words in a vocabulary\n",
    "\n",
    "So One Hot Encoding characters involves vectors that are much shorter than those of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This practical consideration would seem to make the Generative task much harder.\n",
    "\n",
    "It is somewhat amazing that what is generated\n",
    "- Has correctly spelled words/keywords\n",
    "- Is Syntactically correct (sentences end with a \".\", parentheses/brackets are balanced)\n",
    "- Is meaningful: the elements/words are arranged in a logical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even though\n",
    "- We have not explicilty identified any of these concepts\n",
    "- Nor forced training to respect them (via a loss function)\n",
    "\n",
    "Remember\n",
    "- All of this behavior was \"learned\" by identifying the correct next **character**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fake [Shakespeare](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#shakespeare), or fake politician-speak\n",
    "- Fake code \n",
    "- Fake [math textbooks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#algebraic-geometry-latex)\n",
    "- [Click bait headline generator](http://clickotron.com/about)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the generative model\n",
    "\n",
    "Feeding the predicted $\\y_\\tp$ back into the model in order to generate $\\y_{(\\tt+1)}$\n",
    "will require a subtle change during training.\n",
    "\n",
    "At test time, the picture is straightforward:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_one_to_many.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But during training, we may make an incorrect prediction for $\\hat\\y_\\tp$ !\n",
    "\n",
    "This error would cascade, resulting in mis-prediction for all subsequent elements $\\hat\\y_{(\\tt')}$ for $ \\tt' > \\tt$\n",
    "\n",
    "To prevent this, during training\n",
    "- We **don't** feed predicted $\\hat{\\y}_\\tp$ back into the model\n",
    "- We feed the true $\\y_\\tp$ into the model instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called *teacher forcing*\n",
    "\n",
    "$$\n",
    "\\x^\\ip_\\tp = \\y_{(t-1)}\n",
    "$$\n",
    "rather than\n",
    "$$\n",
    "\\x^\\ip_\\tp = \\hat\\y_{(t-1)}\n",
    "$$\n",
    "for $t > t'$.\n",
    "\n",
    "- When extending the sequence\n",
    "- A teacher forces the student (model) to continue with the *correct* answer\n",
    "- Rather than the student's answer\n",
    "- If it didn't do so, once the student (model) predicted incorrectly, it's errors would compound\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training, with Teacher Forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_many_to_many_teacher_forcing.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling from the generative model\n",
    "\n",
    "Remember that a Classifier (the output stage of our model)\n",
    "- generates a *probability distribution* (over the elements of the vocabulary $V$)\n",
    "\n",
    "For the prediction, we usually *deterministically* choose the element of $V$ with highest probability\n",
    "$$\n",
    "\\hat{\\y} = \\argmax{v \\in V} { \\pr{v} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Deterministic choice might not be best for the generative process\n",
    "- One wrong choice propagates to all successive elements of the sequence\n",
    "- The output is always the same ! Boring !\n",
    "\n",
    "So what is usually done is that our prediction is a *sample* from the probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Here is the process in pictures\n",
    "- The training inputs are given in red\n",
    "- The test (inference) time inputs are given in black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Teacher forcing is indicated in red\n",
    "- Predictions $[ \\; \\hat{\\y}_\\tp \\, | \\, 1 \\le \\tt \\le T \\; ]$ **are not** used as input (lower right)\n",
    "- Only correct targets $[ \\; \\y_\\tp \\, | \\, 1 \\le \\tt \\le T \\; ]$ are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The input sequence to the Decoder is modified by\n",
    "- prepending a special \"start of output\" symbol\n",
    "$$\\x_{(-1)} = \\langle \\text{START} \\rangle$$\n",
    "- appending a special \"end of output\" symbol $\\langle \\text{END} \\rangle$ to training examples\n",
    "    - The Decoder stops when it generates the end of output symbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Encoder* is a many to one RNN\n",
    "- Takes the variable length \"seed\" sequence\n",
    "- Outputs a fixed length representation of the seed\n",
    "    - This is one of the strengths of an RNN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Decoder* is a one to many RNN\n",
    "- Takes the fixed length representation of the seed produced by the Encoder\n",
    "    - Used to initialized the Decoder's latent state $\\h_{(0)}$\n",
    "- Outputs a variable length sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative text: limitations\n",
    "\n",
    "The model we described (and will explore in a code example) is absolutely primitive\n",
    "- Predict next *character* rather than next *word*\n",
    "- Simple: one RNN layer\n",
    "- Trained on very small number of examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results of our toy model may not be as impressive as we hoped.\n",
    "\n",
    "Understanding why is important\n",
    "- It will help you to understand what is needed to improve models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Predicting the next character is a limitation compared to predicting the next word\n",
    "    - have to learn syntax as well as semantic concepts\n",
    "    - motivated by limited memory available (need short OHE vectors)\n",
    "- We have limited data on which to train\n",
    "- We have limited time to perform the training\n",
    "    - Back propagation over long sequences is time consuming\n",
    "- We will use very short sequences in our training data\n",
    "    - Because of the limited memory and limited time available\n",
    "    - Can't capture long range dependencies across words\n",
    "        - e..g, matching gender of word to subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Here](https://app.inferkit.com/demo) is a link to a state of the art model\n",
    "that is not subject to the same limitations\n",
    "\n",
    "We will learn about its architecture in a later module.\n",
    "- 3 billion weights !\n",
    "- Trained on 500 billion tokens\n",
    "    - Used \\\\$ 42K of electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
