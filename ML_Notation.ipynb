{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "- Supervised Learning involves supplying a number ($m$) of examples.\n",
    "- Each example is a pair consisting of\n",
    "    - vector $\\x$ consisting of $n \\ge 1$ *features* (attributes)\n",
    "    - scalar (sometimes a vector) $\\y$\n",
    "        - referred to as the *target* value or *label* associated with $\\x$\n",
    "\n",
    "- we use **bold face** to indicate a vector (e.g, $\\x$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We use superscript $\\ip$ to index examples, when we have more than one\n",
    "    - $\\x^\\ip, \\x^{(i')}, i \\ne i'$ are two distinct examples\n",
    "    - denote an element $i$ of a collection of $m$ examples (e.g., $\\x^\\ip$)\n",
    "- We use subscript $j$ to index element $j$ of a vector, e.g., $\\x^\\ip_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- So $\\x^\\ip$ is\n",
    "\n",
    "$\n",
    "  \\x^\\ip = \\begin{pmatrix}\n",
    " \\x^\\ip_1 \\\\\n",
    " \\x^\\ip_2 \\\\\n",
    "  \\vdots  \\\\\n",
    " \\x^\\ip_n\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Each  element of $\\x^\\ip$ is a \"feature\"\n",
    "- $\\x^\\ip_j$ is the $j^{th}$ feature of example $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training set\n",
    "\n",
    "- The collection of examples used for fitting (training) a model is called the *training set*:\n",
    "\n",
    "$$ \\langle \\X, \\y \\rangle= [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ]$$\n",
    "\n",
    "where $m$ is the size of training set and each $\\x^\\ip$ is a feature vector of length $n$.\n",
    "\n",
    "- By seeing many ($m$) pairs of feature vectors and associated labels\n",
    "we will try to infer the correct label $\\y^\\ip$ from the features in $\\x^\\ip$\n",
    "\n",
    "- $\\X$ is an $(m \\times n)$ matrix and $\\y$ is an $(m \\times 1)$ vector of targets.\n",
    "\n",
    "\n",
    "$\n",
    "  \\X = \\begin{pmatrix}\n",
    "  (\\x^{(1)})^T \\\\\n",
    "  (\\x^{(2)})^T\\\\\n",
    "  \\vdots \\\\\n",
    "  (\\x^{(m)})^T \\\\\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    " \\x^{(1)}_1 \\ldots\\x^{(1)}_n \\\\ \n",
    "  \\x^{(2)}_1 \\ldots\\x^{(2)}_n \\\\ \n",
    "   \\vdots \\\\\n",
    "  \\x^{(m)}_1 \\ldots\\x^{(m)}_n \\\\\n",
    "  \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Training set</strong></center>\n",
    "    </tr>\n",
    "<img src=images/mnist_small_train.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We will sometimes add a \"constant\" feature by setting\n",
    "$\\x^\\ip_0 = 1,  0 \\le i \\le m$\n",
    "so that the first column of $\\x$ is $1$:\n",
    "\n",
    "$\n",
    "\\X =\n",
    "\\begin{pmatrix}\n",
    "  1  &\\x^{(1)}_1  & \\ldots &\\x^{(1)}_n \\\\ \n",
    "   1 &\\x^{(2)}_1  &\\ldots  &\\x^{(2)}_n \\\\ \n",
    "   \\vdots & \\vdots & \\ldots &  \\vdots \\\\\n",
    "   1 &\\x^{(m)}_1  &\\ldots  &\\x^{(m)}_n \\\\\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "- So each of the $m$ rows is an example and each of the $n$ columns is a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not just numbers !\n",
    "\n",
    "The features *aren't restricted to be numeric* !\n",
    "\n",
    "In this course, we will deal with data that is\n",
    "- numeric\n",
    "- categorical\n",
    "- text\n",
    "- image\n",
    "- sound (not this course)\n",
    "\n",
    "Of course, you'll have to encode this data as numbers in order for numerical algorithms to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction\n",
    "\n",
    "- Given training example $\\x^\\ip$, we construct a function $h$ to predict its label\n",
    "\n",
    "$$\\hat{\\y}^\\ip = h(\\x^\\ip)$$\n",
    "- We use a \"hat\" to denote predictions: $\\hat{\\y}^\\ip$\n",
    "- The function $h$ will often be parameterized (by $\\Theta$) so, for clarity, we should write\n",
    "\n",
    "$$\\hat{\\y}^\\ip = h(\\x^\\ip; \\Theta)$$\n",
    "- We will often drop $\\Theta$ for ease of reading.\n",
    "- Since $h$ is a function, it should also be possible to make a prediction for a vector $\\mathbf{x}$ that is **not** part of the training set.\n",
    "- That is, we are able *generalize* to non-training examples:  to make out of sample predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L4_S9_Intro_training_2.png\" width=\"60%\"/></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key task of Machine Learning is finding the \"best\" values for parameters $\\Theta$.\n",
    "\n",
    "The process of using training examples $\\X$ to find $\\Theta$\n",
    "- is called *fitting* the model\n",
    "- is solved as an optimization problem (to be described)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Fitting a Linear Regression model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L4_S11_Terminology_training_linear_regr.png\" width=\"50%\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Summary**\n",
    "- a training example is a pair $(\\x^\\ip,\\y^\\ip)$ drawn from training set $\\langle \\X, \\y \\rangle$ consisting of \n",
    "    - a feature vector $\\x^\\ip$ of length $n$\n",
    "    - the associated label (target) $\\y^\\ip$\n",
    "    - $\\X$ is of dimension $m \\times n$\n",
    "    - $\\y$ is dimension $m \\times 1$, i.e., target is a single, continuous value per example\n",
    "- predictions are indicated with a \"hat:\n",
    "    - $\\hat{\\y}^\\ip$ is the prediction made given $\\x^\\ip$ as input\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss/Cost, Utility\n",
    "\n",
    "- The prediction $\\hat{\\y}^{(i)}$ for example $\\x^\\ip$ is perfect if it matches the true label $\\y^\\ip$\n",
    "\n",
    "$$ \\hat{\\y}^\\ip = \\y^\\ip$$\n",
    "\n",
    "- Perfection  is hard (at least at first) so we need a measure for \"how far off\" the prediction is.\n",
    "\n",
    "- We will call the distance between $\\hat{\\y}^\\ip, \\y^\\ip$ the *Loss* (or *Cost*) for example $i$:\n",
    "\n",
    "$$\n",
    "\\loss^\\ip_\\Theta =  L( \\;  h(\\x^\\ip; \\Theta),  \\y^\\ip \\;) = L( \\hat{\\y}^\\ip , \\y^\\ip) \n",
    "$$\n",
    "\n",
    "where $L(a,b)$ is a function that is $0$ when $a = b$ and increasing as $a$ increasingly differs from $b$.\n",
    "\n",
    "Two common forms of $L$ are Mean Squared Error (for Regression) and Cross Entropy Loss (for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The Loss for the entire training set is simply the average (across examples) of the Loss for the example\n",
    "\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L4_s15_Intro_training.jpg\"\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas Loss describes how \"bad\" our prediction is, we sometimes refer to the converse -- how \"good\" the prediction is.\n",
    "\n",
    "We call the \"goodness\" of the prediction  the *Utility* $U_\\Theta$.\n",
    "\n",
    "So we could state the optimization objective either as\n",
    "\"minimize Cost\" or \"maximize Utility\".\n",
    "\n",
    "By convention, the DL optimization problem is usually framed as one of minimization (of cost or loss) \n",
    "rather than maximization of utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since Cost is inversely related to Utility, you will sometimes see\n",
    "the minimization objective written as\n",
    "\"minimize -1 times Utility\".\n",
    "\n",
    "So be forewarned that you will often see Loss function with leading \"negation\" signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Loss functions is a key part of Deep Learning\n",
    "\n",
    "As you will come to see, particularly for Deep Learning, the essence of many problems is in creating a Loss Function that captures the objective of your problem.\n",
    "\n",
    "This is  far from a trivial part of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting/Training a Model\n",
    "\n",
    "The best (optimal) $\\Theta$ is the one that minimizes the Average (across training examples) Loss\n",
    "\n",
    "$$\n",
    "\\Theta^* = \\argmin{\\Theta} { \\loss_\\Theta }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "- The goal of fitting/training is to solve for the $\\Theta$ that minimizes the training set loss \n",
    "$L_\\Theta$ \n",
    "- The method for finding $\\Theta$ is called optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The dot product: Template matching\n",
    "\n",
    "- The \"dot product\" (special case of inner product) is one function\n",
    "that often appears in template matching\n",
    "\n",
    "- It measures the\n",
    "similarity of two vectors\n",
    "\n",
    "$$\n",
    "\\mathbf{v} \\cdot \\mathbf{v}' = \\sum_{i=1}^n \\mathbf{v}_i \\mathbf{v}'_i\n",
    "$$\n",
    "\n",
    "- As a similarity measure (rather than as a distance) high dot product means \"more similar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are several intuitions for the dot product\n",
    "\n",
    "- The dot product is maximized  when large (resp., small) values appear in similar positions in both vectors\n",
    "  - this becomes even more obvious if we $0$-center both vectors such that \"small\" values become negative\n",
    "  - this looks like the statistical formula for covariance\n",
    "    - if we normalize both vectors to unit length, then this looks like correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can generalize dot product to higher dimensions\n",
    "- Compute pair-wise product of corresponding entries\n",
    "- Reduce to a scalar by summing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
